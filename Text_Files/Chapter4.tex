% !TEX root = ../Main.tex

\chapter[Simulating Quantum Circuits with Stabilizer Rank]{Simulating Quantum Circuits with the\\ Stabilizer Rank Method}\label{chap:simulator}
% Look at combining the previous two chapters
% Build stabilizer decompositions, then simulate circuits efficiently for each term
% Here, present methods for operating on the decompositions, for practical simulation
\section{Introduction}\label{sec:simulator_intro}
% Focus on not asymptotic overhead, but implrementations of simulators
Previously, we have discussed decompositions of quantum computations, where each individual term can be efficiently simulated classically. This connection to classical simulation gives an easy operational interpretation to these decompositions, and suggests a way of building a classical simulator along these lines. In this chapter, we will make this connection explicit, introducing methods that can be used to simulate universal quantum circuits, and discussing their implementations.\par
% Use for benchmarking, verification, and prototyping and development
As discussed, efficient classical simulation of quantum effects is broadly believed to be intractable. Nonetheless, classical simulations play an important role in the research and development of quantum technologies. In recent years, fas quantum hardware has continued to improve, an increasingly important role of simulations has been to support the transition and adoption of quantum technology. Providing classical simulators as a test bed enables the development of software engineering, protocols and applications that take into account non-classical features, even while access to actual quantum devices is still limited. \par
For example, \texttt{SimulaQron} and \texttt{NetSquid} are classical simulations of quantum communications networks, developed as part of an effort to promote the development of practical quantum communications~\cite{Dahlberg2017,NetSquid}. These tools have been used to develop proposals for link layer protocols in quantum networks, which can then be tested in the lab~\cite{Dahlberg2019}.\par
In the context of quantum computing, many classical simulators in use today form part of `Quantum Development Kits' (QDKs), software environments for the development of quantum software. These tools broadly follow a similar architecture to that of \texttt{ProjectQ}, described in~\cite{Haner2018}. Typically, the user-facing component is a `high-level' description of a quantum programme, either as an API or with a domain-specific language (DSL), which is agnostic to how it will be evaluated. These programmes can be built out of algorithms and meta-algorithms, such as the Variational Quantum Eigensolver; subroutines and operations, such as the quantum Fourier transform; or even individual gates. The resulting description of the programme can then be compiled to a quantum circuit, and either simulated clasically, or else dependent on requirements further compiled and dispatched to a quantum processor. Multiple such QDKs have been developed over the past 5 years, and a brief summary of the some of the available options is shown in Table~\ref{tab:qdks}.
\begin{table}[H]
\begin{tabular}{|l|c|c|c|}
\toprule
Framework & \makecell{High-level\\Description} & \makecell{Classical\\ Methods} & \makecell{Supported\\ Hardware} \\ \midrule
Microsoft QDK~\cite{MicrosoftQDK} & \texttt{Q\#}~\cite{Svore2018} & State vector & None \\\midrule
\texttt{ProjectQ}~\cite{Steiger2016} & DSL & State vector & IBMQ~\cite{IBMQ} \\\midrule
\texttt{Qiskit}~\cite{Qiskit} & \makecell{QASM~\cite{Cross2017},\\ \texttt{Python} API} & Various & IBMQ~\cite{IBMQ}\\\midrule
\texttt{Circ}~\cite{GoogleCirc} & \texttt{Python} API & \makecell{State vector,\\density matrix} & Bristlecone\footnote{These chips are not currently publicly accessible.}~\cite{CircAnnouncement}\\\midrule
\texttt{Forest}~\cite{RigettiForest} & \makecell{Quil,\\ Python API~\cite{Smith2016}} & \makecell{State vector \\ density matrix} & Rigetti QPU~\cite{RigettiQPU}\\ \bottomrule
\end{tabular}
\caption{A non-exhaustive list of different quantum software frameworks or QDKs. We note that many of these frameworks have additional components aimed at supporting application development that are not mentioned here.}\label{tab:qdks}
\end{table}
% Qiskit, Circ etc use statvector methods
In practice, most of the QDKs mentioned above make use of what could be described as `textbook' classical simulations of quantum computing, where a circuit is simulated by matrix multiplication of the unitary associated with each gate, acting on either a state-vector or a density matrix description~\cite{Nielsen2000}.\par
These simulators have the advantage that they are relatively straightforward to implement, and can leverage mature computational libraries for matrix maths such as \texttt{Numpy}~\cite{Numpy}. Probabilities in the computational basis can also be trivially obtained, either by reading off the right diagonal entry in the density matrix or computing the absolute value squared of the corresponding amplitude in a state-vector. Noise in these models is also relatively straightforward to model, either by using a stochastic noise model inserting extra operators into the circuit in the state-vector case, or else applying Kraus operators directly in the density matrix case~\cite{Nielsen2000}.\par
However, the main drawback to these simulators is their spatial complexity. A state-vector requires $2^{n}$ complex numbers to define, and a density matrix requires up to $2^{2n}$, where $n$ is the number of qubits. As each complex number requires two $64$-bit floating point numbers to specify, the memory requirements can quickly approach the limits of personal computing. A simulation on $30$ qubits requires $16$GB of memory, and up to $45$ qubits this requires $0.5$PB~\cite{Haner2016}. The current top-ranked supercomputer in the world has access to $2.7$PB of memory, meaning it could simulate up to $47$ qubits using these methods~\cite{Top500}.\par
These classical representations also have a significant temporal overhead. In the most straightforward implementation, applying gates requires multiplying $2^{n}\times 2^{n}$ matrices. These updates require time $2^{2n}$ in the state-vector case, and $2^{4.746n}$ for density matrices. In practice though, significant optimizations are possible that can make state-vector simulators reasonably performant at accessible sizes. For example, the $2^{n}\times 2^{n}$ matrices representing single qubit gates are sparse, with the vast  majority of entries being $0$. Other optimizations that have been applied include parallelising single-qubit gate updates~\cite{Smelyanskiy2016,Khammassi2017,Smelyanskiy2016,QiskitAer}, optimising permutation operations such as CNOT and Pauli $X$~\cite{Khammassi2017}, replacing certain arithmetic operations with classical equivalents~\cite{Haner2016}, and accelerating algorithms using parallel execution via \texttt{OpenMP}, \texttt{MPI} or GPUs~\cite{Jones2018,Smelyanskiy2016,Khammassi2017,QiskitAer}.\par
% Mention the other role, of verification and benchmarking
In practice, this limit of approximately $30$ qubits when simulating circuits with personal computers roughly corresponds to the kind of quantum programmes that can be run on current publicly accessible devices, which have anywhere from $5$-$20$ qubits~\cite{IBMQ,RigettiQPU}. However, with continued development of quantum hardware into the 50--72 qubit range~\cite{IBM50,GoogleBristlecone}, classical simulations need to be pushed further, to continue in their other role in the verification and benchmarking of quantum devices.\par
Given the expected intractability of classical simulations at large enough system sizes, the question of verifying quantum computations without simulation constitutes a separate branch of research based on the idea of `interactive proofs'~\cite{Aharonov2017,Mahadev2018}. Nonetheless, classical simulations offer a unique opportunity in verification as at any point the simulation can be paused and the system state inspected. Large-scale classical simulations also provide a performance baseline, as part of attempts to establish Quantum Supremacy.\par
% Other simulators usually special purpose, optimised to do better to fit this role
Recent work has focused on tensor-network methods, as introduced in Section~\ref{sec:intro_classical_desc}, to push classical simulations up past $45$ qubits, and have achieved some of the largest scale classical simulations to date~\cite{Pendault2017,Chen2018,Chen2018b,Markov2018,Villalonga2019}. These papers on large classical simulations focus on on simulating quantum circuits on grids of qubits with local connectivity. This restriction is motivated by the designs of current quantum processors, and also allows for specific optimizations that reduce the temporal complexity of the simulation. State of the art methods typically split this grid into sub-blocks, which are locally contracted leaving only connections between blocks~\cite{Pendault2017,Chen2018b,Markov2018,Villalonga2018}. The remaining $s$ contractions are then `sliced', fixed to one of $2^{s}$ values and then contracted fully~\cite{Pendault2017}. This has a natural operational interpretation in terms of a sum-over-paths expansion~\cite{Markov2018}, and has the advantage that the contractions within blocks can be parallelised.\par
These methods all achieve runtimes that scale as $\max \left[ 2^{dl}, 2^{n}\right]$, where $l$ is the length of the longest edge of the grid~\cite{Markov2005}. They also have exponential spatial requirements, though these are reduced compared to a state-vector method by virtue of tensor slicing. Through application of supercomputing resources, these methods have simulated random universal circuits of up to depth $40$ on $72$ qubits~\cite{Villalonga2018}, depth $35$ on $100$ qubits~\cite{Chen2018}, and depth $24$ on $121$ qubits~\cite{Villalonga2019}.\par
Because this generated of quantum hardware aims to  maximize qubit count, it will not employ full error-correction routines. As a result, noise is a significant factor in the system, and limits the depth of circuits that can be run. We refer to this regime of quantum computing as `Noisy Intermediate Scale Quantum' or NISQ~\cite{Preskill2018}. Thus, much like system size for the state-vector simulator, the exponential simulation overhead in the depth does not render the simulations intractable. In fact, simulators can benefit from the increased noise level, by dropping terms from the simulation and reducing the overall computational time required by a constant factor, as discussed in Section~\ref{sec:srank_discussion}.
\section{Results}
In the rest of this chapter, we will discuss a distinct method for simulating universal quantum circuits, based on stabilizer state decompositions. We will present simulation results for several types of quantum circuit, and argue that this method has a great potential for simulating circuits on current and near-term quantum hardware.\par
\subsection{Methods for Manipulating Stabilizer Decompositions}\label{sec:manipulating_decompositions}
At a high level, simulating a quantum circuit $U$ using a decomposition into efficiently simulable terms requires two main stems. Firstly, we need to build a representation of the circuit state $U\ket{x}$ for an input state $\ket{x}$, which is itself part of our efficiently representable set of states. Then, we need a routine for computing output variables from the distribution, either computing explicit probabilities if we interested in strong simulation, or else sampling from the output distribution if we are interested in weak simulation.\par
In the following, we will use $\mathtt{U}$ to denote a classical description of the quantum circuit $U$. We store $\mathtt{U}$ as a sequence of gates, where each gate includes its label, e.g.\ `H', and the labels of the qubits it acts on.\par
Stabilizer states will be encoded classically using either the CH or the DCH representations, introduced in Chapter~\ref{chap:stabilizers}.
\subsubsection*{Building Decompositions}
The main method for constructing a stabilizer state decomposition, given a description of a quantum circuit $U$, is the PBC method introduced in~\cite{Bravyi2015} and~\cite{Bravyi2016}, and outlined previously in Sections~\ref{sec:pbc} and~\ref{sec:pbc_decomposition}. We will review the method briefly here, with a focus on implementation in software. \par
Implementing a PBC requires rewriting $U$ as an equivalent Clifford circuit $V$. We achieve this by walking through the circuit $\texttt{U}$, and replacing each of the $m$ non-Clifford gate with an appropriate magic state or states, and state-injection gadget, such as the example shown in Figure~\ref{fig:t_inject}. We note that this requires a library of known gadgets for implementing different gates. The result is a new circuit $U'$, acting on $n$ qubits and $m$ magic states.\par
State-injection gadgets include additional, measurement controlled `correction' operations. By post-selecting on these measurement-outcomes, we can expand out $U'$ as a sum of different Clifford circuits $V_{y}$
\[U\ket{x} = \sum_{y}\matrixel{y}{V_{y}}{x\otimes \psi} \]
where $y$ is the post-selection string with length $O(m)$, and $\ket{\psi}$ is the joint state of all the magic states.\par
It was shown in~\cite{Bravyi2016} that given some approximate stabilizer state decomposition of the magic states $\ket{\tilde{\psi}}$, we can construct a PBC to sample from the output distribution of the circuit by sampling the post-selection string at random. Thus, for each gadget, we sample the measurement outcomes appropriately to build-up the Clifford circuit $\mathtt{V}$.\par
As previously discussed, when injecting a gate $U$ the correction operation has the form $UPU^{\dagger}$ for some Pauli operator $P$. If $U\in\mathcal{C}_{3}$, then by definition $UPU^{\dagger}$ is a Clifford operator and we are done. Otherwise, we will need to introduce additional layers of state-injection until we build an all-Clifford circuit $\mathtt{V}$.\par
Finally, we need to construct an approximate stabilizer state decomposition for the magic states $\ket{\psi}$. In general, $\ket{\psi}$ will be a tensor-product of different `species' of magic state, and so we build the full approximation using the multiplicative upper bound
\[\chi_{\epsilon}\left(\ket{\psi}\right) = \chi_{\epsilon}\left(\ket{T}^{\# T}\right)\,\chi_{\epsilon}\left(\ket{CCX}^{\# CCX}\right)\,\chi_{\epsilon}\left(\ket{\theta}^{\#\theta}\right)\cdots\]
For Clifford magic states, we can make use of the random codes construction. Otherwise, we can use sparsification. We note that this again implies a library of best-known decomposition strategies for each magic-state we introduce.\par
Overall then, the gadgetization method takes as input a classical description of an $n$-qubit circuit $\mathtt{U}$ and target error $\epsilon$, and returns a new description of a Clifford circuit $\mathtt{V}$ acting on $n$ qubits and $m$ magic states, the corresponding post-selection string $\va{y}$, and an approximate stabilizer state decomposition $\mathtt{\ket{\tilde{\psi}}}$. A pseudo-code description of this method is given in Algorithm~\ref{alg:gadgetize_decomposition}.\par
\large{\itshape{The Sum-over-Cliffords picture}}\par
The PBC model has an interesting feature where the number of qubits in the stabilizer state expansion depends only on the magic states, and not on the number of qubits in the circuit. Stabilizer circuits are efficient to simulate in terms of the number of qubits, but the $O(n^{3})$ overhead is still considered significant in practice. Thus, if there are fewer magic states, the PBC can reduce the number of variables in the simulation. But, in general, universal quantum computations have a number of gates that scales as $\poly\left(n\right)$, and gadgetization will result in more qubits.\par
An alternative strategy for building stabilizer state decompositions makes use of the equivalence between stabilizer circuits and stabilizer states. If we consider a Clifford gate decomposition $Q=\sum_{i}\alpha_{i}V_{i}$, then the action of $Q$ on a stabilizer state results in a stabilizer state decomposition
\begin{equation}
Q\ket{\phi} = \sum_{i}\alpha_{i}V_{i}\ket{\phi} = \sum_{i}\alpha_{i}\ket{\phi_{i}},
\end{equation}
which we can then turn into an approximation stabilizer state decomposition with sparsification, giving a decomposition with a rank $O(\norm{\va{\alpha}}^{2})$.\par
From this, we can define a notion of `extent' for a unitary
\begin{equation}
\xi\left(Q\right) = \min_{V}\norm{\va{\alpha}}^{2} \;:\;Q=\sum_{i}\alpha_{i}V_{i}.
\end{equation}
For example, considering single-qubit rotations in around the $Z$ axis of a Bloch sphere with $\theta\in[0,\pi/2$, we can expand them into two Clifford branches
\begin{equation}
R_{Z}\left(\theta\right) = \left(\cos{\frac{\theta}{2}}-\sin{\frac{\theta}{2}}\right)I + \mathe^{-\mathi\pi/4}\sqrt{2}\sin{\frac{\theta}{2}}S,
\label{eq:rotation_expansion}
\end{equation}
with corresponding extent $\xi\left(R\left(\theta\right)\right)=\left(\cos{\frac{\theta}{2}}+\tan{\frac{\pi}{8}}\sin{\frac{\theta}{2}}\right)^{2}$~\cite{Bravyi2018}. Similar results can be found for all $Z$ rotations, where we slightly adjust the phase and the Clifford operations on each branch.\par
This expansion corresponds with the stabilizer extent of the $\ket{T}$ state by setting $\theta=\frac{\pi}{4}$. In fact, it was shown by Earl Campbell that for injectable Clifford magic states, such as $\ket{T}$ and $\ket{CCZ}$, that the extent-optimal stabilizer state decomposition can be used to `lift' a Clifford gate expansion of the corresponding unitary (i.e.\ $T$ and $CCZ$), that is also optimal~\cite{Bravyi2018}.\par
Using submultiplicativty, we can thus upper-bound the stabilizer extent of the circuit $U$ as
\begin{equation}
\xi\left(U\right) = \prod_{i=1}^{m}\xi\left(U_{i}\right)
\end{equation}
for each non-Clifford gate $U_{i}$. We can then build up a term in the stabilizer state decomposition by iterating through $\mathtt{U}$. If the gate is Pauli or Clifford, we just apply it and update the state. Otherwise, for each non-Clifford gate $U_{i}$ we sample a branch from the Clifford expansion with $p_{i,j}=\frac{\left|\alpha_{i,j}\right|}{\norm{\va{\alpha_{i}}}}$ as in the sparsification method, and apply the corresponding Clifford gate $V_{i,j}$. We can repeat this $O(\xi\left(U\right))$ times, to produce a stabilizer state decomposition of the $U\ket{x}$. This algorithm is outlined in Algorithm~\ref{alg:soc_decomposition}.\par
\begin{algorithm}[p]
\begin{algorithmic}
\Require{Known set of gadgets for non-Clifford gates.}
\Function{GadgetDecomposition}{$\mathtt{U}, \epsilon$}
    \State{ $\mathtt{V}\gets \emptyset$ \Comment{Output Clifford circuit}}
    \State{ $\ket{\psi}\gets \emptyset$ \Comment{Magic states}}
    \For{$\mathtt{U_{i}}\in \mathtt{U}$}
        \If{$U_{i} \notin \mathcal{C}_{2}$}
            \State{ Sample a measurement outcome $z$}
            \State{ $\mathtt{V}\gets \mathtt{V}\cap\mathtt{G}\cap\mathtt{V}_{z}$} \Comment{$\mathtt{G}$ is the gadget for $U_{i}$.}
            \State{ $\ket{\psi}\gets\ket{\psi}\otimes \psi_{G}$ \Comment{Magic state associated with $\mathtt{G}$}}
        \Else
            \State{ $\mathtt{V} \gets \mathtt{V}\cap \mathtt{U_{i}}$}
        \EndIf
    \EndFor
    \State{ Reorder qubits in $\mathtt{V},\ket{\psi}$ to join common species of magic state}
    \State{ $\ket{\tilde{\psi}}=\emptyset$}
    \For{$\ket{\psi_{U_{j}}^{\otimes \# U_{j}}}\in\ket{\psi}$}
        \State{ $\ket{\tilde{\psi}}\gets \ket{\tilde{\psi}}\otimes \ket{\tilde{\psi_{U_{j}}}}$ \Comment{Rank is set by $\epsilon$.}}
    \EndFor
    \State{\Return{$\mathtt{V}$, $\ket{\tilde{\psi}}$}}
\EndFunction
\end{algorithmic}
\caption{Pseudocode description of the computational routine for construction a stabilizer state decomposition of a quantum circuit using state-injection gadgets.}\label{alg:gadgetize_decomposition}
\end{algorithm}
\begin{algorithm}[p]
\begin{algorithmic}
\Require{Clifford decompositions of non-Clifford gates.}
\Function{SumOverCliffordDecomposition}{$\mathtt{U}, \epsilon, \ket{x}$}
    \State{$\ket{\tilde{\psi}} = \emptyset$}
    \State{$\xi\gets$ \Call{ComputeExtent}{$\mathtt{U}$}}
    \State{$i\gets 0$}
    \While{$i<\chi_{\epsilon}= O\left(\xi \epsilon^{-2}\right)$}
        \State{$\ket{\phi} \gets\ket{x}$}
        \State{$c\gets 1$}
        \For{$U_{i}\in\mathtt{U}$}
            \If{$U_{i}\notin \mathcal{C}_{2}$}
                \State{Sample Clifford branch $j$ of gate $U_{i}$}
                \State{$\ket{\phi}\gets V_{i,j}\ket{\phi}$}
                \State{$c\gets \frac{\alpha_{i,j}}{\left|\alpha_{i,j}\right|}\, c$}
            \Else
                \State{$\ket{\phi}\gets U_{i}\ket{\phi}$}
            \EndIf
        \EndFor
        \State{$\ket{\tilde{\psi}}\gets \ket{\tilde{\psi}}+c\ket{\phi}$}
        \State{$i\gets i+1$}
    \EndWhile
    \State{\Return $\ket{\tilde{\psi}}$}
\EndFunction
\end{algorithmic}
\caption{Pseudocode description of building stabilizer state decompositions in the sum-over-Cliffords picture.}\label{alg:soc_decomposition}
\end{algorithm}
\begin{algorithm}[p]
\begin{algorithmic}
\Require{$L$, number of samples to take, $n$, number of qubits, $\Pi$, Pauli projector}
\Function{NormEstimation}{$\Pi$, $\ket{\tilde{\psi}}$}
    \State{$\va{\eta} \gets \{\eta_{i}=0\}$}
    \State{$\{\ket{\eta_{i}}\}\gets $\Call{RandomEquatorialState}{n}}
    \For{$\alpha_{i},\;\ket{\phi_{i}}\in\ket{\tilde{\phi}}$}
        \State{$\Gamma\gets 1$}
        \For{$P\in\Pi$}
            \State{$\Gamma_{P},\ket{\phi_{i}} \gets$\Call{MeasurePauli}{$P$,$\ket{\phi}}$}
            \If{$\Gamma_{P}=0$}
                \State{$\Gamma\gets 0$, Break loop}
            \EndIf
            \State{$\Gamma\gets\Gamma\,\Gamma_{P}$}
        \EndFor
        \If{$\Gamma \neq 0$}
            \For{$\ket{\eta_{i}}\in \{\ket{\eta_{i}}\}$}
                \State{$\eta_{i}\gets \Gamma\,\alpha_{i}\,\braket{\eta_{i}}{\phi_{i}}$}
            \EndFor
        \EndIf
    \EndFor
    \State{\Return{$\frac{2^{n}}{L}\sum_{i}\left|\eta_{i}\right|^{2}$}}
\EndFunction
\end{algorithmic}
\caption{Pseudocode outline of the Norm Estimation routine for computing expectation values of Pauli projectors.}\label{alg:norm_estimation}
\end{algorithm}
\begin{algorithm}
\begin{algorithmic}
\Require{$n$, number of qubits}
\Function{MetropolisSampling}{$\ket{\tilde{\psi}}$, $m$}
\State{$\va{x}\gets $ Random initial $n$-bit binary string}
\State{$p_{x}\gets \left|\sum_{i}\alpha_{i}\braket{x}{\phi_{i}}\right|^{2}$}
\For{$j\in [1,\dots,m]$} \Comment{$m$ repetitions of the random walk step}
    \State{$j\gets$ Random integer $\in [1,\dots,n]$}
    \State{$\va{x}' \gets \va{x}\oplus \va{e}_{j}$}
    \State{$p_{x'}\gets \left|\sum_{i}\alpha_{i}\braket{x'}{\phi_{i}}\right|^{2}$}
    \If{$p_{x}=0$} \Comment{Always move away from $0$ amplitudes}
        \State{$x\gets x'$, $p_{x}\gets p_{x'}$}
    \Else
        \State{Generate $r\in[0,1)$ uniformly at random}
        \If{$r<\frac{p_{x'}}{p_{x}}$} \Comment{Always accept if $p_{x'}>p_{x}$}
            \State{$x\gets x'$, $p_{x}\gets p_{x'}$}
        \EndIf
    \EndIf
\EndFor
\State{\Return{$\va{x}$}}
\EndFunction
\end{algorithmic}
\caption{Pseudocode description of the Metropolis-style Monte Carlo method for sampling a computational basis string $x$ from the output distribution of a stabilizer state decomposition.}\label{alg:metropolis}
\end{algorithm}
\subsubsection*{Output Variables}
There are two main methods for computing output variables from a given stabilizer state decomposition. The first is the `norm estimation' routine, introduced in~\cite{Bravyi2016} and refined in~\cite{Bravyi2018}. Norm estimation can be used to compute measurement probabilities, and also to sample as described in Section~\ref{sec:pbc_decomposition}. The second is a Metropolis-style Monte Carlo method, which can be used to return samples in the computational basis. Both methods were developed by Sergey Bravyi, and we introduce them here with a view to their implementation. The two methods are also outlined in Algorithms~\ref{alg:norm_estimation} and~\ref{alg:metropolis}, respectively.\par
\large{\itshape{Norm Estimation}}\par
This routine allows us to quickly compute an approximation to $\norm{\psi}$. Importantly, given a projector $\Pi$, we can compute the probability of that outcome as
\begin{equation}
p\left(\Pi\right) = \frac{\norm{\Pi \psi}^{2}}{\norm{\psi}^{2}}.
\end{equation}
In particular, it is possible to show that if we generate equatorial stabilizer states $\ket{\eta}$ uniformly at random, then the random variable $\eta\equiv 2^{n/2}\left|\braket{\eta}{\psi}\right|$ has the property that
\[
\begin{array}{l r}
\mathbb{E}\left(\eta^{2}\right) = \norm{\psi}^{2} & \mathbb{E}\left(\eta^{4}\right) \leq 2\norm{\psi}^{4}
\end{array}
\]
and thus, the average inner product of $\ket{\psi}$ with equatorial stabilizer states is the norm, with variance at most $\norm{\psi}^{4}$~\cite{Bravyi2018}.\par
The number of samples we need depends on the accuracy desired. It can be shown that given an estimate 
\[\bar{\eta}=\frac{1}{L}\sum_{i}\left|\eta_{i}\right|^{2}\]
then $\bar{\eta}$ approximates $\norm{\psi}$ to within $\epsilon$ relative error $\bar{\eta}=\left(1\pm \epsilon\right)\norm{\psi}$ with probability $\frac{3}{4}$, provided $L=4\epsilon^{-2}$~\cite{Bravyi2018}. We can of course increase this probability to $1-\delta$ by taking $\log{\delta^{-1}}$ estimates of $\bar{\eta}$.\par
In Section~\ref{sec:dch_ch_methods}, we introduced an algorithm for computing inner products between stabilizer states $\ket{\phi}$ and equatorial stabilizer states. This method has computational complexity $O(n^{3})$. Thus, given a stabilizer state decomposition $\ket{\tilde{\psi}}$, we can use compute $\norm{\tilde{\psi}}$ in time $O(L\chi n^{3})$, where $L$ is the number of samples of $\eta$.\par
As part of the sampling routine described in Section~\ref{sec:pbc_decomposition}, we want to compute marginal probabilities $P\left(x_{1},x_{2},\dots x_{j}\right)$ for some $j$-bits. These marginals correspond to fixing $j$ qubits, and projecting the rest onto a $2^{n-j}$ dimensional codespace, generated by $j$ Pauli operators~\cite{Bravyi2016}. This codespace can be generated by $j$ Pauli operators, giving
\[\Pi = \prod_{i=1}^{j}\frac{1}{2}\left(I+P_{i}\right)\]
where $P_{i}$ are $n$ qubit Pauli operators. We can thus apply this projector by measuring each of the Pauli generators in turn. Recall that each Pauli measurement takes time $O(n^{2})$, (c.f.~\ref{sec:dch_ch_methods}) and thus computing $\norm{\Pi\tilde{\psi}}^{2}$ also has runtime $O\left(L\chi n^{3}\right)$.\par
To avoid accumulation of errors, each marginal probability needs to be computed with multiplicative error $O(w^{-1})$ when sampling from $w$ output bits. Using the bound on the approximation accuracy above, this implies $L=O(w^{2})$. As there are $w$ marginals to compute, sampling with the norm estimation method thus takes time $O(\chi n^{3}w^{3})$.\par
In the gadgetized picture, we can employ norm estimation by first setting the measurement projector $\Pi$, and then updating it to obtain the corresponding PBC $\Pi_{s}$ by conjugating the projector with the Clifford circuit $\mathtt{V_{y}}$. These Pauli updates can be computed efficiently classically, using similar update rules as for a stabilizer tableau~\cite{Aaronson2004}. Otherwise, for decompositions obtained using the sum-over-Cliffords method, no further preprocessing is required.\par
We note that norm estimation is required to compute individual computational basis amplitudes, and to convert a stabilizer state decomposition into the state vector picture. Recalling that in the CH and DCH representations, we can compute $\braket{x}{\phi}$ in time $O(n^{2})$, this means that for a given stabilizer state decomposition we can compute $\braket{x}{\tilde{\psi}}$ in time $O(\chi n^{2})$. To avoid potential floating point errors, stabilizer states in the decomposition are stored only with their relative phase coefficients. Thus, these amplitudes needs to be reweighted by $\norm{\tilde{\psi}}$.\par
\large{\itshape{Metropolis Estimation}}\par
One advantage of norm estimation is that it can be used to compute the probability of arbitrary Pauli measurements. However, as discussed, while technically polynomial it has a runtime up to $O(n^{6})$ in the number of qubits. Thus, an alternative strategy based on Metropolis Monte Carlo methods was proposed by Sergey Bravyi, that also makes use of the ability to compute individual computational basis amplitudes in time $O(n^{2})$.\par
The idea is to define a random walk through the set of computational basis strings, flipping one bit at a time and computing the amplitude of the new string. If at some time-point we have computational basis string $x$ and amplitude $\left|\braket{x}{\tilde{\psi}}\right|$, then we obtain $x'$ by flipping a single bit at random, and compute $\left|\braket{x'}{\tilde{\psi}}\right|$. If the new amplitude is larger, we accept the move. Otherwise, we accept with fixed probability
\[p = \frac{\left|\braket{x'}{\tilde{\psi}}\right|}{\left|\braket{x}{\tilde{\psi}}\right|}.\]
It can be argued that, assuming that for all strings $x, y$ there exists a path of single-bit moves between them, the steady state distribution of this walk converges to the output distribution of the circuit in time $\poly{n}$~\cite{Bravyi2018}.In practice, we have used this method to obtain samples from the output distribution on $50$ qubit circuits with mixing time of $\sim 2000$ steps~\cite{Bravyi2018}. Importantly, once the chain has been mixed, we can then obtain samples by continuing to run the core random-walk step (contained within the \texttt{For} loop in Algorithm~\ref{alg:metropolis}) for a further $s$ repetitions, recording the string $\va{x}$ at the end of each step as one sample.\par
In general, computing amplitudes requires time $O\left(\chi n^{2}\right)$, combining the contribution from each term. While we store an un-normalised description of the approximate state $\ket{\tilde{\psi}}$, here we can avoid the need to perform norm estimation as we are interested in ratios of amplitudes, and so the norms cancel. We might expect then that the Metropolis method to have a runtime that scales as $O\left(\chi n^{2}\right)$. However, we can actually remove a factor of $n$ from the runtime of the random-walk step by exploiting the fact we are flipping single bits at a time.\par
Recall from Section~\ref{sec:dch_ch_methods} that we compute the computational amplitudes $\va{x}$ in the CH and DCH picture by commuting a Pauli $X(\va{x})$ past the CH/DCH layers which we denote here as a Clifford circuit $W$. We can store this resulting Pauli operator $P'=W^{\dagger}X(\va{x})W$, which takes $O(n^{2})$ to compute, for a constant memory cost. We can then compute the operator $Q'$, obtained by commuting $X(\va{x'})$, as
\[Q' = WX(\va{x'})W^{\dagger} = WX(\va{e}_{j})X(\va{x})W^{\dagger} = WX(\va{e}_{j})W^{\dagger} P'.\]
Because $X(\va{e}_{j})$ acts as the identity everywhere except qubit $j$, commuting this operator through the Clifford layer can be optimised to ignore any terms except for those involving qubit $j$. By inspection of Eqs.~\ref{eq:dch_dupdate},~\ref{eq:dch_cupdate},\ref{eq:dch_hupdate} and~\ref{eq:tableau_update_pauli}, this takes time $O(n)$ as each vector-matrix multiplication will involve only a single row or column.\par
Thus, overall then, if we run the Metropolis method for time $m+s=T$ to obtain $s$ samples, the runtime scales as $O\left(\chi n^{2}\right)+O\left(T\chi n\right)$.
\subsection{Implementation of the Simulator}\label{sec:implementing_simulator}
% Extend the simulators of Chapter 2 w/ wrapper
To implement these simulation methods, the fundamental data-structures we consider are arrays of stabilizer states, and their complex coefficients. The stabilizer states themselves are encoded using either the CH or the DCH representation, as each encoding supports the necessary update routines including fast inner-product calculations with equatorial states.\par
Building on the existing implementations of the CH and DCH simulators discussed in Section~\ref{sec:stabilizer_simulators}, the simulator was written in \texttt{C++}. In the previous section, we introduced two distinct approaches for building stabilizer state decompositions, and two distinct methods for computing output variables. Thus, the simulator was designed using the `strategy' design pattern~\cite{GoF}, which allows different algorithms for the same task to be used interchangeably.\par
The core of the simulator is a class we call \texttt{Runner}, which is responsible for maintaining the stabilizer state decomposition. The \texttt{Runner} class is initialized with the target stabilizer rank, the number of qubits and, optionally, the initial stabilizer state $\ket{x}$.\par
Because the specifics of building a stabilizer state decomposition will depend extensively on the circuit, including factors like the choice of gadget or Clifford decomposition, the \texttt{Runner} accepts user-defined strategies. These can be implemented using `function objects', classes that can be called like functions~\cite{CPPRefFunctionObject}. This allows the decomposition strategy to have internal state information, e.g.\ the choice of `subspaces' used to decompose Clifford magic states, which is kept separate from the resulting stabilizer state decomposition. The user defines their decomposition strategy by sub-classing the \texttt{DecompositionBuilder} class, and at runtime the \texttt{Runner} class simply calls the function object $\chi$ times to build up the decomposition. The \texttt{Runner} method then also implements both the norm estimation and metropolis methods.\par
Details of the specific strategies used to build stabilizer state decompositions will be given as part of the descriptions of the method in Section~\ref{sec:circuit_simulations}.\par
In their implementation, the DCH and CH classes have the same set of public methods, differing only in their internal representation of the stabilizer state. We formalise this using the notion of `template' programming~\cite{GoF,CPPRefTemplates}. Templates allow the implementation of the simulator to be agnostic to the choice of internal representation. The choice of encoding is made at compile-time, by specifying either the \texttt{CHState} or \texttt{DCHState} classes.\par
\subsubsection*{Parallelization}
% Building Decompositions and Sampling Outputs is intrinsically trivially parallelizable
An important feature of all the routines outline in Algorithms~\ref{alg:gadgetize_decomposition}--\ref{alg:metropolis} is that they each include a step where we operate on every single term in the stabilizer state decomposition independently. In the decomposition routines, each stabilizer state term is built up separately. Similarly, in the output routines, we use a `map-reduce' model where the same calculation is applied to every state before combining the results at the end; for example, computing a probability amplitude requires summing the value of $\braket{x}{\phi_{i}}$ for every state.\par
These kind of computations are called `embarrassingly parallelisable', as there is little to no dependency between the tasks, and thus they can be easily sped-up by providing multiple parallel workers. Importantly, these loops are also they only parts of the computation where the complexity scales as $O(\chi)$; other steps, such as gadgetizing a circuit or computing a PBC, are efficiently computable. Thus, these parallelisable loops dominate the runtime, and by Amdahl's law we can significantly reduce the runtime of the programme by adding parallel workers~\cite{Amdahl1967}.\par
% Locally parallised w/ OpenMP
% Distributable w/ MPI
In contrast to `data parallelism', such as the SIMD methods discussed in Section~\ref{sec:stabilizer_discussion}, this kind of computation is called Multiple Instruction Multiple Data(MIMD) computation~\cite{Flynn1972}. MIMD programmes can be further subdivided into `shared memory' execution, where parallel threads run on a single computer, or `distributed' execution, where multiple separate processes run independently on multiple processing units.\par
Shared memory parallelism is the most straightforward to implement. The programme is mainly executed by a single thread, with additional parallel threads `forked' from the programme for specific subroutines~\cite{Kessler2007}. In \texttt{C++}, this can be implemented using \texttt{OpenMP}, which allows parallelising loops and `map-reduce' operations with single-line annotations~\cite{OpenMP}.\par
However, the benefits of shared memory parallelism are limited by the kind of hardware available, in particular the maximum memory and number of threads. While this kind of parallelism is sufficient for personal computers, scaling the simulator to large problem sizes requires distributed memory techniques.\par
We used a `message passing' model of distributed memory parallelism, where multiple processes each execute a unique copy of the programme, and synchronise and share results through inter-process communication~\cite{Kessler2007}. In particular, we use \texttt{Open-MPI}, an open source implementation of the Message Passing Interface standard~\cite{OpenMPI,MPIForum}.\par
We implement a subclass of \texttt{Runner}, called \texttt{MPIRunner}, for distributed memory computations. On initialization, each process is assigned a `rank' in the group, with the rank-0 process designated the `master'~\cite{MPIForum}. All processes run the same setup steps to initialize the simulation, and the `master' process then splits the decomposition, allocating a unique fraction of states $f_{i}$ to each of the `worker' processes. The worker processes then perform computations locally on their share of the decomposition. Initialization is done entirely locally, with the only communication being to pause the programme until all processes have computed their terms~\cite{MPIForum}. For output variables, processes again apply the `map-reduce' model locally, before broadcasting their results to the master process which performs a final reduction step~\cite{MPIForum}. We can also allow for `hybrid' parallelism, where each distributed process also uses local, shared-memory execution to speed up its part of the simulation task.\par
Through distributed memory execution, the stabilizer rank simulator can be scaled up to even larger problem sizes. In this thesis, the largest simulation we considered used $32\mathrm{GB}$ of memory, running on UCL's Myriad supercomputing cluster, but this method could be scaled to even larger instances.
\subsubsection*{Integration with \texttt{Qiskit-Aer}}
% Also possible to integrate this runner w/ Qiskit Aer
Using the \texttt{Runner} class outlined above, we were also able to integrate our simulation method with \texttt{Qiskit-Aer}, the component of IBMs \texttt{Qiskit} QDK that is responsible for classical simulations. Here, we briefly outline the \texttt{Qiskit} execution model, and show how our simulator can be incorporated with it.\par
The fundamental data-structure in \texttt{Qiskit} is the \texttt{Qobj} or `Quantum Object', which contains information about a quantum programme in the form of the available quantum and classical registers, and the circuits to be run. The \texttt{Qobj} is then converted to Javascript Serial Object Notation (JSON), such that it can be transmitted over the internet to the IBM Quantum Experience, or dispatched to the \texttt{Aer} suite of classical simulators.\par 
% Circuits parsed by qiskit, build decomposition and then do measurements w/ MC
This classical backend also employs a version of the strategy pattern. The \texttt{Qobj} is first parsed by a \texttt{Controller}, which is responsible for setting up the simulation, including configuring the shared-memory parallel execution, and creating an internal representation of the quantum circuit as an sequence of \texttt{Gate} objects. This includes reading configuration options related to the choice of strategy, or else picking a strategy automatically by inspecting the memory requirements for the circuit. The \texttt{Controller} class is also responsible for implementing noisy simulations using a stochastic noise model, where additional random gates and measurements are inserted according to a specified noise model. It does this by sampling additional gates, and inserting them into the circuits. The controller then initializes a \texttt{State} class for each circuit in the \texttt{Qobj}, passing in the details of the circuit and quantum and classical registers. \par
We integrate the stabilizer rank simulator by creating a custom \texttt{State} class. These objects are responsible for parsing the quantum circuit, and maintaining an internal representation of the quantum state called a \texttt{qreg} or `quantum register' object. In our case, the \texttt{qreg} object is a version of the \texttt{Runner} class. We begin by first iterating through the circuit, checking it contains only gates we now how to decompose, computing the (multiplicative upper-bound) on the circuit extent $\xi$, and initializing the \texttt{Runner} with $\chi_{\epsilon}=\lceil \xi \epsilon^{-2}\rceil$ copies of the initial stabilizer state $\ket{x}$.\par
The simulation strategy then depends on whether the circuit contains intermediate measurements, whether as a result of sampled noise operators or just as part of the circuit to be run. If there are no intermediate measurements, then the simulation is embarrassingly parallel up until the final measurement stage. Thus, we can iterate through the circuit in parallel, building up each term in the decomposition using the sum-over-Cliffords method.\par
Otherwise, we need to coordinate the simulator at each measurement operation. Thus, we instead build up the circuit one gate at a time. For each gate, we then begin a parallel loop, taking $\chi_{\epsilon}$ samples of the Clifford branches for that gate. When we reach a measurement step, we then run the metropolis method to produce a single sample, and apply the corresponding Pauli projector to decomposition, again parallelising over the $\chi{\epsilon}$ terms. This model is less performant than the previous case, as it requires blocking the computation until all parallel workers have finished, and also as there it requires entering and exiting parallel execution multiple times, which has some associated overhead.\par
The current integration as part of \texttt{Qiskit-Aer} only uses the metropolis method, as this is the most general method for sampling from the output distribution of the circuit. However, the output distribution of some circuits will not satisfy the irreducibility requirement. We can in practice achieve good performance, passing the benchmark suite of test circuits for \texttt{Qiskit}, by re-mixing the metropolis method for each sample. This avoids us becoming stuck in a non-zero amplitude, and returning the same bit-string for $100\%$ of the samples.\par
This version of the simulator was made public in April, 2019, in the \texttt{0.1.0} release of \texttt{Qiskit-Aer}. As an example of the capabilities of our simulator, we ran a small random circuit benchmark using both the default \texttt{Qasm} simulator of \texttt{Qiskit-Aer}, which is based on the state-vector method, and our simulator, which is called \texttt{extended\_stabilizer} in the \texttt{Qiskit-Aer} package. We used the default parameters for the stabilizer rank-based method, which sets $\epsilon=0.05$ and mixes the Metropolis method for $3000$ steps.\par
We generated random circuits with a fixed number of non-Clifford gates, and simulated these circuits $10$ times each with both methods, running on the UCL Myriad cluster with access to $4$ $2.3\mathrm{GHz}$ processors and $16\mathrm{GB}$ of RAM, and a maximum of $90$ minutes of compute time. These conditions are intended to simulate typical personal computers. We then recorded the runtime, and plotted the `speedup' as the ratio of $\frac{\text{Extended Stabilizer Runtime}}{\text{Qasm Runtime}}$. The results are shown in Figure~\ref{fig:qiskit_race}.
\begin{figure}[t]
\centering
\begin{scaletikzpicturetowidth}{0.75\textwidth}
    \input{Figures/QiskitRace/comparison}
\end{scaletikzpicturetowidth}
\caption{Figure comparing the runtime of stabilizer rank-based simulations to the \texttt{Qiskit} state-vector simulation for random circuits, using \texttt{Qiskit-Aer}. The solid red region corresponds to the regime where quantum circuits required too much memory to simulate with state-vector methods.}\label{fig:qiskit_race}
\end{figure}
As we can see, the runtime of the stabilizer-rank based methods increases with the number of non-Clifford gates, which we expect as the extent also increases. However, even for circuits with small extent, below $20$ qubits the stabilizer rank method requires significantly increased runtime compared to the state-vector approach. However, as the number of qubits continues to increase, the stabilizer rank method becomes increasingly efficient.\par
Above a hard upper limit of $30$ qubits, the spatial requirements of the state-vector simulator exceed the available memory, and so the circuits cannot be simulated. The stabilizer rank simulator, however, is still capable of running the simulation. it is also important to note that, as expected, the runtime of the stabilizer state method does not increase significantly with the number of qubits. For example, a circuit on $50$ qubits and with $20$ non-Clifford gates required on average $246$ seconds to simulate with the stabilizer-rank method, compared to the $213$ seconds required by the state-vector method to simulate a similar circuit on $30$ qubits.
\subsection{Simulations of Quantum Circuits}\label{sec:circuit_simulations}
% Results of simulations performed with this method
In this section, we will present results using the stabilizer rank method to simulate three common classes of quantum circuit: `oracle' or black-box circuits, variational methods, and random circuits.
\subsubsection*{Hidden Shift Circuits}
% Used to verify the Simulator
Oracle-based circuits are a very common technique for designing quantum algorithms, encompassing everything from toy methods such as the Deutsch-Jozsa algorithm up to famous algorithms like Grover search and Shor's algorithm~\cite{Mosca2008}. These methods generally involve initializing the quantum state in a superposition of computational basis states, and then applying a black-box unitary $O_{f}$ that computes some classical function $f$~\cite{Nielsen2000}.\par
The hidden-shift problem is an example of a computational task where quantum algorithms require fewer invocations or queries to the oracle than any classical method~\cite{Roetteler2008}. Consider a `bent' boolean function $f(\va{x}):\mathbb{Z}_{2}^{n}\rightarrow \pm 1$, which has the property that its Fourier coefficients $\hat{f}\left(\va{w}\right)=\frac{1}{2^{n}}\sum_{\va{x}}\left(-1\right)^{\va{w}\cdot \va{x} + f\left(\va{x}\right)}$ are equal for all $n$-bit strings $w$.\par
For any boolean function, we can also define a shifted function $f_{s}$ as $f_{s}\left(x\right)=f\left(x\oplus s\right)$, where $\va{s}\in\mathbb{Z}_{2}^{n}$ is a fixed binary string. Finally, we can also define the Fourier transformed dual of the bent function as~\cite{Roetteler2008}
\[ \tilde{f}\left(\va{x}\right) = 2^{-n/2}\sum_{\va{y}\in\mathbb{Z}_{2}^{n}}\left(-1\right)^{\va{x}\cdot \va{y}}f\left(y\right) \]
Given oracles $O_{f_{s}}$ and $O_{\tilde{f}}$ that will evaluate both the shifted function and its unshifted dual for some input string $\va{x}$, it will take a classical method $O(n)$ queries to determine the hidden-shift string $\va{s}$. However, a quantum algorithm can determine $\va{s}$ in just two queries.
\begin{figure}[H]
\centerline{
\Qcircuit @C=1em @R=.7em {
    \lstick{\ket{0^{\otimes n}}} & \gate{H^{\otimes n}} & \gate{O_{f_{s}}} & \gate{H^{\otimes n}} & \gate{O_{\tilde{f}}} & \gate{H^{\otimes n}} & \rstick{\ket{s}} \qw \\
}}
\caption{Circuit diagram of the quantum method for solving the hidden-shift problem, described in~\cite{Roetteler2008}.}
\end{figure}
It is interesting to note that, if we further restrict this problem to only have access to $f_{s}$ and $f$, and not the dual bent function, there nonetheless exists an alternative quantum algorithm capable of solving for $\va{s}$ in $O(n)$ queries. The authors conjecture that in this case a classical method would require an exponential number of queries~\cite{Roetteler2008}.\par
The specific class of bent functions considered in~\cite{Roetteler2008} are called Majorana-McFarland functions. In practice, we can construct a quantum oracle for random bent functions from this family using a fixed number of $CCZ$ gates. This method was outlined in Appendix~F of~\cite{Bravyi2016}, which used the hidden-shift problem to benchmark the performance of the stabilizer rank simulation method. Because we specify the string $s$ in the construction of the oracle, this method has `built-in' verification that the simulator is running correctly.\par
In particular,~\cite{Bravyi2016} detail how to construct a bent function starting from a random boolean function $g:\mathbb{Z}_{2}^{n/2}\rightarrow \pm 1$, which they generate using a random sequence of $Z$ and $CZ$ gates and a fixed number of $CCZ$ gates. If we denote this circuit $O_{r}$, then the oracles for the hidden-shift problem are defined as
\begin{equation}
\begin{array}{l r}
 O_{f_{s}} = \left[\left(\prod_{i=1}^{n/2}CZ_{i,i+n/2}\right)I\otimes O_{r}\right] Z\left(\va{s}\right) & O_{\tilde{f}} = \left(\prod_{i=1}^{n/2}CZ_{i,i+n/2}\right)O_{r}\otimes I
\end{array}
\end{equation}
For $m$ $CCZ$ gates, the overall circuit thus contains $2m$ non-Clifford gates. In~\cite{Bravyi2016}, they simulate these circuits by using a gadget for $CCZ$ gates built out of $4$ $T$ gates. Here, we use the hidden-shift circuits on $40$ qubits as a way to test our results on decomposing alternate Clifford magic states, and the sum-over-Cliffords picture. In particular, we simulate the hidden-shift circuits using four distinct methods
\begin{itemize}
    \item A gadgetized decomposition, using $4$ $\ket{T}$ magic states to synthesis each $CCZ$ gate
    \item A gadgetized decomposition, using $\ket{CCZ}$ magic states directly
    \item A sum-over-Cliffords decomposition, using 4 $T$ gates per $CCZ$ gate
    \item A sum-over-Cliffords decomposition, using $CCZ$ gates
\end{itemize}
This allows us directly compare the sum-over-Cliffords and gadgetized methods, and to compare direct decompositions over Clifford+$T$ synthesis.\par
The simulation was developed in collaboration with Mark Howard, based on the previous simulations developed by David Gosset in~\cite{Bravyi2016}. The setup for the simulation, including constructing the oracle circuits, and constructing the PBC projectors in the gadgetized method, are implemented in \texttt{MATLAB}.\par
As previously stated, sampling from $n$ output bits with the norm estimation routine has a runtime that scales as $O\left(\chi_{\epsilon}n^{6}\right)$. To circumvent this, we exploit the fact that we can clasically cache the state of the simulation before measurement, and that the output state of the simulation is an approximation of single output string $\ket{s}$, and learn the string $\va{s}$ by sampling single qubits a time. An example of this is shown in Figure~\ref{fig:hs_readout}. Overall, this method takes time $\left(\chi_{\epsilon}n^{4}\right)$ to sample from all $n$ bits.\par
\begin{figure}[t]
\centering
\includegraphics[width=0.75\textwidth]{Figures/HiddenShiftHistogram.pdf}
\caption{Figure showing the probability $P\left(x_{i}=1\right)$ for $20$ bits, obtained using the sum-over-Cliffords methods and synthesizing the $CCZ$ gate with $4$ $T$ gates. Each output probability is computed individually.}
\label{fig:hs_readout}
\end{figure}
To cache the decomposition between each norm estimation step, we store the choice of subspace or Clifford branches in MATLAB. We then make use of the \texttt{MEX} API to pass this data, and the Pauli projectors to be applied, to the \texttt{C++} simulator. This computes and returns the probability $p_{x_{i}=1}$, and we then sample bits by generating uniform random numbers $r\in [0,1)$ and returning $1$ iff $r<p_{x_{i}=1}$.\par
In the gadgetized case, the number of terms in the decomposition depends on the stabilizer fidelity and the target infidelity, which we will denote here as $\delta$. Using the stabilizer fidelity of the $\ket{T}$ and $\ket{CCZ}$, then for a bent function using $m$ $CCZ$ gates the size of the decomposition $\chi$ scales as
\begin{equation}
\begin{array}{l r}
F\left(T\right)\approx 0.853 & \chi = \lfloor\frac{4 F\left(T\right)^{-8m}}{\delta}\rfloor \approx \lfloor 4\left(\frac{3.57^{m}}{\delta}\right)\rfloor\\ 
F\left(CCZ\right)\approx \frac{9}{16} & \chi = \lfloor\frac{4 F\left(CCZ\right)^{-2m}}{\delta}\rfloor \approx \lfloor 4\left(\frac{3.16^{m}}{\delta}\right)\rfloor\\ 
\end{array}
\end{equation}
Similarly, for the sum-over-Cliffords method, the number of terms is given by the stabilizer extent, and the target error $\epsilon$. For $m$ $CCZ$ gates, the number of terms is given by
\begin{equation}
\begin{array}{l r}
\xi\left(T\right) \approx 1.17 & \chi = \lceil 1.17^{8m} \epsilon^{-2}\rceil = \lceil 3.57^{m}\epsilon^{-2}\rceil \\
\xi\left(CCZ\right) = \frac{16}{9} & \chi = \lceil 1.78^{2m}\epsilon^{-2}\rceil = \lceil 3.16^{m}\epsilon^{-2}\rceil
\end{array}
\end{equation}
Recall that for Clifford magic states, the stabilizer fidelity and stabilizer extent coincide, which explains the correspondence in the scaling between the random codes and sum-over-Cliffords method. In all the simulations, we set $\delta=\epsilon=0.3$. Due to platform limitations, the \texttt{C++} component was executed serially, though decompositions were built in parallel using \texttt{MATLAB}'s built in parallel execution. All simulations were run on a dual-core $1.9\mathrm{GHz}$ Intel Xeon, with $32\mathrm{GB}$ of RAM.\ The results of the simulations are shown in Figure~\ref{fig:hs_results}.
\begin{figure}[H]
\centering
\caption{Figures demonstrating the performance of the stabilizer rank method on simulating the hidden shift problem, using $4$ methods of building the stabilizer state decomposition. Figures originally created for~\cite{Bravyi2018}}\label{fig:hs_results}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{HiddenShiftTimes_RC.pdf}
\caption{Runtime of the hidden shift simulation as a function of the number of $CCZ$ gates, using the random codes method to build decompositions, using Clifford+$T$ synthesis and direct decomposition.}\label{fig:hs_rc_times}
\vspace{2.5cm}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{Figures/HiddenShiftTimes_SoC.pdf}
\caption{Runtime of the hidden shift simulation as a function of the number of $CCZ$ gates, using the sum-over-Cliffords method to build decompositions, comparing Clifford+$T$ synthesis with $CCZ$ gates.}\label{fig:hs_sc_times}
\end{subfigure}
% \end{figure}
% \begin{figure}[p]\ContinuedFloat
\begin{subfigure}[t]{0.65\textwidth}
\includegraphics[width=\textwidth]{Figures/HiddenShiftErrors.pdf}
\caption{The maximum observed approximation error in single-qubit probabilities over all bits in the hidden-shift string. The colour and markers correspond to the timing plots above. Not shown are two data-points in the $T$ random-code methods, for $14$ and $16$ Toffoli gates, with large errors $0.304$ and $0.512$ respectively.}
\end{subfigure}
\end{figure}
\subsubsection*{QAOA}
As mentioned in Section~\ref{sec:simulator_intro}, current NISQ computers lack full error correction and thus accumulate noise as the circuit depth increases. Thus, there is a great deal of interest in relatively low-depth quantum algorithms that can run on NISQ devices. The main class of these algorithms are `variational methods', hybrid quantum-classical algorithms with applications in optimization and quantum computational chemistry~\cite{Moll2018}.\par
In general, variational methods use a simple processing scheme where the quantum computer is used to prepare an `ansatz' state using a low-depth circuit. The gates in the circuit are typically parameterized. We then perform a series of measurements, and these outcomes are post-processed by a classical algorithm. This can be iterated, where the parameters of the ansatz state are updated by the classical algorithm, to converge to a heuristic solution~\cite{Preskill2018,Moll2018}.\par
The Quantum Approximate Optimization Algorithm (QAOA) is an example of a variational method, applied to classical combinatorial optimization problems~\cite{Farhi2014}. These kind of optimization problems are usually specified by a number of boolean clauses, and we are interested in optimizing a function `satisfied clauses'
\[C\left(\va{z}\right) = \sum_{\alpha} C_{\alpha}\left(\va{z}\right),\]
where each sub-clause $C_{\alpha}$ acts on a subset of bits from the full $n$-bit string $\va{z}$, and evaluates to either $\{0,1\}$ or $\pm 1$ depending on the definition of the problem~\cite{Farhi2014}. A clause is said to be `satisfied' if it evaluates to $1$. Examples of combinatorial optimization problems include MaxSat, where we are tasked with finding the string $\va{z}$ that satisfies the most clauses simultaneously.\par
Given a clause $C_{\alpha}$, we can define an operator $\hat{C}_{\alpha}$ by replacing the bits $z_{\alpha_{i}}$ in the clause with Pauli $Z$ operators, and in turn we can define $\hat{C}=\sum_{\alpha}\hat{C}_{\alpha}$~\cite{Farhi2014}. Different computational basis strings are eigenstates of this operator, such that 
\[\hat{C}\ket{\va{z}}=C\left(\va{z}\right)\ket{\va{z}}.\]
The QAOA algorithm proceeds by preparing an ansatz state parameterized by $2p$ angles $\beta_{i}$ and $\gamma_{i}$, for some fixed value of $p$. The system is initialized in the ground state $\ket{g}$ of the operator $\hat{C}$, which depends on the definition of the problem but is typically a trivial assignment such as $\ket{0^{\otimes n}}$ or $\ket{+^{\otimes n}}$~\cite{Farhi2014,Farhi2014b}. We then apply $p$ rounds of a pair of parameterised rotation operators
\[\begin{array}{lr}
U_{C}\left(\gamma_{i}\right)=\mathe^{-\mathi \gamma \hat{C}} &  U_{B}\left(\beta_{i}\right) = \prod_{j}\mathe^{\mathi\beta_{i} X(\va{e}_{j})}
\end{array}.\]
For each paramaterised sate $\ket{\psi_{\va{\gamma},\va{\beta}}}$ has been prepared, we can determine the expectation value of the $\hat{C}$ operator 
\[E_{\va{\gamma},\va{\beta}}=\matrixel{\psi_{\va{\gamma},\va{\beta}}}{\hat{C}}{\psi_{\va{\gamma},\va{\beta}}}.\]
Importantly, it can be shown that as $p\rightarrow \infty$, the maximum of this expectation value corresponds to the maximum of $C\left(\va{z}\right)$~\cite{Farhi2014}. The authors further show that even with $p=1$, reasonable results that some classical strategies can be obtained~\cite{Farhi2014,Farhi2014b}.\par
We consider the application of QAOA to a combinatorial optimization problem called MaxE3Lin2, where QAOA has been shown to outperform random classical guesses at $p=1$~\cite{Farhi2014b}. We choose this problem over MaxCut as it requires slightly fewer rotations, but nonetheless contains a significant number of non-Clifford gates. In particular, we consider randomly generated instances of MaxE3Lin2 with $50$ variables and $66$ clauses, requiring $50$ qubits and $66$ Pauli $Z$ rotations.\par
The goal of MaxE3Lin2 is to maximize an objective-function
\[C\left(\va{z}\right) = \frac{1}{2}\sum_{1\leq u < v < w \leq n} d_{uvw}z_{u}z_{v}z_{w}\]
where each clause acts on $3$ variables~\cite{Farhi2014b,Bravyi2018}, and the coefficients $d_{uvw}=\{0,\pm1\}$. The number of clauses is given by the number of non-zero coefficients. When generating the problems, restrict ourselves to instances with fixed degree $4$, such that each qubit appears in at most $4$ terms. We then prepare a state 
\[\ket{\psi_{\gamma,\beta}} = U_{B}\left(\beta\right)U_{C}\left(\gamma\right)\ket{+^{\otimes n}}\]
with two parameters, using a single round of rotations~\cite{Farhi2014b}.\par
Classical preprocessing methods exist for estimating $E_{\gamma,\beta}$, which can be used to speed-up the classical step of the variational algorithm. In particular, we use a method that allows the expectation variables of a sparse Hamiltonian with `computationally tractable' states, states which can be efficiently specified in the computational basis~\cite{VandenNest2009}. This allows us to approximately compute
\[\matrixel{\psi_{\gamma}}{U_{B}\left(\beta\right)^{\dagger} \hat{C}U_{B}\left(\beta\right)}{\psi_{\gamma}}\]
with additive error $\epsilon$ in time $O(m^{4}\epsilon^{-2})$~\cite{Bravyi2018}. Figure~\ref{fig:mc_qaoa} plots the estimates of $E_{\gamma,\beta}$ for a particular instance of MaxE3Lin2.\par
In their original paper, Farhi et al.\ fix $\beta=\pi/4$. This has the advantage that the rotation $U_{B}$ becomes a Clifford operator 
\[\mathe^{-\mathi \pi/4 X\left(\va{e}_{i}\right)}=H\left(\va{e}_{i}\right)S\left(\va{e}_{i}\right)H\left(\va{e}_{i}\right),\]
meaning all non-Clifford terms arise from the $Z$ rotations. We can also see from Figure~\ref{fig:mc_qaoa} that the line from $\beta=\pi/4$ passes through both a minima and a maxima of $C\left(\va{z}\right)$. Thus, in our simulation, we also fix $\beta=\pi/4$. The cost function is symmetric about $\gamma=0$, and so we sweep $\gamma$ from $0$ to $\pi$.
\begin{figure}[b]
\centering
\begin{scaletikzpicturetowidth}{0.7\textwidth}
\input{Figures/QAOA/MCPlot}
\end{scaletikzpicturetowidth}
\caption{Heat-map showing the expectation value $E_{\beta,\gamma}$ for the simulated instance of MaxE3Lin2, generated using the method of~\cite{VandenNest2009}, implemented in \texttt{MATLAB}.}\label{fig:mc_qaoa}
\end{figure}
Each rotation has a sum-over-Cliffords expansion~\cite{Bravyi2018}
\[\mathe^{-\mathi \frac{\gamma}{2} d_{uvw}Z_{u}Z_{v}Z_{w}} = \begin{cases}
\alpha I + \beta CNOT_{u,v}CZ_{v,w}S_{v}S_{w}CNOT_{u,v} & d_{uvw} = 1\\
\alpha I + \beta \mathi CNOT_{u,v}CZ_{v,w}S^{\dagger}_{v}S^{\dagger}_{w}CNOT_{u,v} & d_{uvw} = -1\\
\end{cases}\]
where the coefficients $\alpha$ and $\beta$ are the phase terms associated with each branch
\[\begin{array}{lr}
b_{0} = \mathe^{\mathi \gamma}-\mathi & \alpha = \frac{b_{0}}{\left|b_{0}\right|}\\
b_{1} = 1 - \mathe^{\mathi \gamma} & \beta = \frac{b_{1}}{\left|b_{1}\right|}\\
\end{array}.\]
For each value of $\gamma$, we build the corresponding stabilizer state decomposition with
\[\chi = \lceil \xi\left(\gamma\right)\epsilon^{-2}\rceil = \lceil \left(\left|b_{0}\right|+\left|b_{1}\right|\right)^{2m}\epsilon^{-2}\rceil \]
terms for $m$ clauses. We then run the Metropolis method to take $40000$ samples from the output distribution of the state $\ket{\psi_{\gamma}}$, and compute an estimate of the expectation value
\begin{equation}
E_{sim}\left(\gamma\right)\frac{1}{40000}\sum_{s=1}^{40000}C\left(z_{s}\right).
\end{equation}
The simulations were run on the UCL Legion supercomputing cluster, running on Dell C6100 compute nodes, using a shared-memory parallelism model with $12$ parallel threads and $24\mathrm{GB}$ of RAM.\ We ran these methods with $\epsilon=0.1$ and $0.15$, and compare our estimates to the results from the heuristic method of~\cite{VandenNest2009}. Results are shown in Figure~\ref{fig:qaoa_results}.
\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.8\textwidth}
\begin{scaletikzpicturetowidth}{\textwidth}
\input{Figures/QAOA/expectation_estimate}
\end{scaletikzpicturetowidth}
\caption{Plot comparing the estimates of $E_{\gamma}$ obtained using the methods of~\cite{VandenNest2009}, with the estimates obtained using the sum-over-Cliffords simulator and sampling from the output distribution with the Metropolis method.}\label{fig:qaoa_results_comparison}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\begin{scaletikzpicturetowidth}{\textwidth}
\input{Figures/QAOA/chi}
\end{scaletikzpicturetowidth}
\caption{Plot showing how the stabilizer rank of the decomposition $\xi\left(\gamma\right)\epsilon^{-2}$, as a function of the QAOA parameter $\gamma$.}\label{fig:qaoa_srank}
\end{subfigure}
\caption{Graphs showing the results of the sum-over-Cliffords simulations of a $50$-qubit instance of MaxE3Lin2 with $66$ clauses. We use the same plotting code as in~\cite{Bravyi2018}, where the error in $E_{sim}\left(\gamma\right)$ is computed using the methods of~\cite{Wolff2004}.}\label{fig:qaoa_results}
\end{figure}
\subsubsection*{Random Circuit Models}
% Introduce cicuit model, supremacy test
The final simulation task we consider are random circuit models. It is well known that the output distribution of Haar random unitaries satisfy a property called anti-concentration~\cite{Hangleiter2017}, and that random quantum circuits also satisfy this property for sufficient depth~\cite{Aaronson2016,Emerson2098,Harrow2017}. Random circuit models like this are not computationally useful but, as discussed in Section[INSERT], satisfy complexity theoretic that make them hard to sample from using classical simulation.\par
Here, we consider a class of random circuits introduced by the Google AI group, referred to as either `Google Circuits' or as `Qubit Speckle'~\cite{Aaronson2016,Martinis2016}. Circuits are built up using alternating layers of entangling two-qubit gates, and randomly placed single qubit gates, either Clifford rotations $\mathe^{-i\frac{\pi}{4} X}=HSH$, $\mathe^{-\frac{\pi}{4} Y}=S^{\dagger}HSHS$, or the $T$ gate~\cite{Boixo2016}. These choices are designed to try and frustrate commuting gates through the circuit to, for example, combine $T$ rotations and cancel them to reduce the overall depth of the circuit~\cite{Harrow2017}. This gate-set also has the property that it forms an approximate unitary $t$-design, and thus that the problem of sampling from their output distributions satisfies both the average-case hardness~\cite{Bouland2018,Movassagh2018} and anti-concentration~\cite{Brandao2010,Hangleiter2017} criteria sought for a test of quantum supremacy.\par
% Clear signature of 'quantumness' in XE
These circuits have the property that with increasing depth, their output distribution converges to the Porter-Thomas distribution~\cite{Boixo2016}. Based on this property, the authors introduce a metric called the cross-entropy difference that quantifies the accuracy of a given sample of $m$ bitstrings from the output distribution of a random circuit~\cite{Boixo2016}.
\begin{equation}
\alpha = \log{2^{n}} +\gamma -\frac{1}{m}\sum_{j=1}^{n}\log\left(\frac{1}{p_{U}\left(\va{x}_{j}\right)}\right)
\label{eq:cross_entropy}
\end{equation}
where $\gamma$ is the Euler-Mascheroni constant, and $p_{U}\left(\va{x_{j}}\right)$ is the probability of the output string string $\va{x}_{j}$~\cite{Boixo2016}. This quantity has the property that $\alpha=1$ for an ideal sample, and $\alpha=0$ if the sample is from a uniform distribution. Recalling the discussion Section [INSERT REF], we can thus define two distinct classical tasks as part of a quantum supremacy test: `cross-entropy benchmarking' (XEB), where we compute the probabilities $p_{U}$, and `heavy output generation' (HOG), sampling from the output distribution of the circuit $U$~\cite{Aaronson2016,Villalonga2019}.\par
The depth of the circuit required to achieve $\alpha=1$ depends on the locality restrictions of two-qubit gates. For example, if we can perform two-qubit gates between arbitrary qubits, then the distribution will converge to Porter-Thomas with a depth $O(\log{n})$ in the number of qubits~\cite{Emerson2098,Boixo2016}. If instead we are limited to two-qubit interactions only between neighbouring qubits on a $2$D lattice, then the depth required scales as $O(\sqrt{n})$~\cite{Harrow2018}.\par
As discussed Section~\ref{sec:simulator_intro}, large scale simulations of Google circuits have focused on lattices with $2$D connectivity, a restriction which is driven by comparisons with current and future quantum hardware which also uses qubits connected on a $2$D grid. The simulation techniques employed also exploit this locality restriction in their design~\cite{Pendault2017,Chen2018b,Markov2018,Villalonga2019}. These simulators are capable of both the XEB and HOG tasks~\cite{Villalonga2019}.\par
Our simulation method, in contrast, makes no restrictions on qubit connectivity in its design. Here, we will explore the feasibility of using the stabilizer rank method for the HOG task. We introduce an extension of the Google circuits to different connectivities, and examine the stabilizer extent and simulation runtime as a function of the circuit depth.\par
Google's random circuits are constructed with the following method
\begin{enumerate}
    \item Initialize the system in the $\ket{+^{\otimes n}}$ state.
    \item Apply $CZ$ gates to a subset of qubits, following a `CZ Schema'.
    \item Apply single-qubit gates from the set $\{\mathe^{-i\frac{\pi}{4}X},\mathe^{-i\frac{\pi}{4}Y},T\}$, according to one of two rules.
    \item Repeat steps $2$ and $3$ $d-1$ more times for depth $d$.
    \item Apply a Hadamard gate to each qubit.
    \item Sample in the computational basis.
\end{enumerate}
\begin{algorithm}[b]
\begin{algorithmic}
\Require{$d$-dimensional square lattice graph\\ $G = \{V= \{v_{i}=\left(c_{1,i},\dots,c_{d,i}\right)\}$, $E=\{v_{i},v_{j}\} \}$}
\Require{$N\left(v\right)$, the neighbourhood of $v\in G$.}
\State{$\mathcal{E}\gets\emptyset$ \Comment{Set of visited edges.}}
\State{$S\gets \{ \}$ \Comment{Initialize an empty array $S$.}}
\While{$\mathcal{E}\neq E$}
    \State{$H = \left(V', E'=E\setminus\mathcal{E}\right) \leq G$, \Comment{Graph minor from deleting $\mathcal{E}$}}
    \For{$i \in \{1,\dots,d\}$}
        \State{$\mathcal{L}\gets \emptyset$ \Comment{New layer in the CZ schema}}
        \For{$j \in \{1,\dots\left|d_{i}\right|\}$}
            \For{$v_{k} \in V': c_{i,k}=j$}
                \If{$\{v_{k}, v_{k'}: c_{i,k'}=j+1\}\in E'$}
                    \State{$\mathcal{L}\gets\mathcal{L}\cap \{v_{k}, v_{k'}: c_{i,k}=j+1\}$}
                    \State{$W \gets\{v_{k}, v_{k'},N\left(v_{k}\right),N\left(v_{k'}\right)\}$\Comment{Set of vertices to exclude.}}
                    \State{$H\gets H' = \left(V', E'\right)\leq H$ Minor induced by deleting vertices $W$.}
                \EndIf
            \EndFor
        \EndFor
        \State{$\mathcal{E} \gets \mathcal{E}\cap \mathcal{L}$}
        \State{$S\gets S + \{\mathcal{L}\}$\Comment{Append layer $\mathcal{L}$ to the schema.}}
    \EndFor 
\EndWhile
\end{algorithmic}
\caption{Pseudo-code description of a greedy algorithm for constructing a `CZ Schema', covering every edge in a $d$ dimensional square lattice or `grid' graph. Each axis of the lattice $d_{i}$ has $\left|d_{i}\right|$ points.}\label{alg:cz_schema}
\end{algorithm} 
The `CZ Schema' defines how we place $CZ$ gates. A limitation of current quantum hardware is the inability to reliably apply $CZ$ gates on neighbouring qubits~\cite{Boixo2016,Villalonga2018}. Thus, for each layer of the circuit, we apply a pattern of $CZ$ gates obeying this hardware restriction, and such that for sufficient depth $d$ every qubit is involved it at least one $CZ$ gate. The authors describe $CZ$ patterns for $2$D lattices~\cite{Boixo2016,Villalonga2018}, which are made up of $8$ layers. For each time-step $l$ in the random circuit, the authors use the $CZ$ pattern of layer $l\mod{8}$.\par
We can extend these schema to arbitrary-dimensional connectivity using the method outlined in Algorithm~\ref{alg:cz_schema}. For each layer, we iterate along one dimension of the lattice, greedily adding edges. Each time we add an edge, we drop those vertices and their neighbourhood from being involved in any other CZ gate in that layer. Applying this to a $2$D grid gives the same pattern described in~\cite{Villalonga2018}. Examples of $1$, and $2$D CZ schema are given in Figure~\ref{fig:cz_schema}. For all-to-all connectivity, we instead apply $\frac{fn}{2}$ $CZ$ gates to random pairs of qubits, such that we involve some fraction $f$ of qubits in each layer of the circuit.\par
\begin{figure}[p]
\caption{Examples of CZ schema for different dimensionalities of square qubit lattice. Connected qubits are subject to a $CZ$ gate in that layer, and each layer appears sequentially according to the numbering. In some layers, we have marked qubits excluded by the neighbouring $CZ$ restriction in red.}\label{fig:cz_schema}
\centering
\begin{subfigure}[t]{0.75\textwidth}
\includegraphics[width=\textwidth]{Figures/1d_CZschema.png}
\caption{$CZ$ schema for a $5$-qubit $1$D lattice. Here, we highlight in each layer the qubits excluded by the neighbouring $CZ$ restriction.}
\end{subfigure}
\begin{subfigure}[t]{0.85\textwidth}
\includegraphics[width=\textwidth]{Figures/2D_CZschema.png}
\caption{$CZ$ schema for a $4\times 4$ qubit grid. As described in Algorithm~\ref{alg:cz_schema}, we apply $CZ$ gates along alternating the dimensions of the grid in each layer.}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=\textwidth]{Figures/Algorithm_7_Example.png}
\caption{Example showing how two layers of $2$D schema are built up step-by-step using Algorithm~\ref{alg:cz_schema}, read left-to-right. Striped qubits indicated those excluded by the neighbouring $CZ$ restriction.}
\end{subfigure}
\end{figure}
Previous work has described two distinct rules for placing single-qubit gates. In the first scheme, we place one of the three gates with equal probability on any qubit that was acted on by a $CZ$ gate in the previous layer~\cite{Boixo2016}. However, in this scheme, it is possible to produce configurations like $T\,CZ\,T$, which can be rearranged to cancel the $T$ gates as diagonal unitaries commute. Thus, an updated rule was proposed. The first single-qubit gate applied must always be a $T$ gate. Then, we apply either $\mathe^{-\mathi\frac{\pi}{4}X}$ or $\mathe^{-\mathi\frac{\pi}{4}Y}$ to qubits acted on by $CZ$ in the previous layer, and $T$ if a qubit was acted on by a one of these two rotations on the previous layer~\cite{Villalonga2018}. We use the second rule to place single-qubit gates, except in the $1$D case as otherwise this rule will never place more than a single $T$ gate on each qubit. An example random circuit for a $1$D lattice is shown in Figure~\ref{fig:random_1d_circ}.
\begin{figure}[H]
\centerline{
\Qcircuit @C=1em @R=.7em {
    \lstick{\ket{0}} & \gate{H} & \ctrl{1}\barrier[0.3em]{4} &\qw & \gate{T}\barrier[0.3em]{4}            &\qw & \qw & \qw \\
    \lstick{\ket{0}} & \gate{H} & \control \qw               &\qw & \ctrl{1}                              &\qw & \gate{\mathe^{-\mathi\frac{\pi}{4}Y}} & \qw \\
    \lstick{\ket{0}} & \gate{H} & \qw                        &\qw & \control \qw                          &\qw & \ctrl{1}      & \qw \\
    \lstick{\ket{0}} & \gate{H} & \ctrl{1}                   &\qw & \gate{\mathe^{-\mathi\frac{\pi}{4}X}} &\qw & \control \qw  & \qw \\
    \lstick{\ket{0}} & \gate{H} & \control \qw               &\qw & \gate{T}                              &\qw & \qw           & \qw\\
}}
\caption{Circuit diagram showing $3$ layers of the random circuit applied to a $5$-qubit, $1$D lattice. Each dashed line represents the end of a single layer.}\label{fig:random_1d_circ}
\end{figure}
We examined the performance and resource requirements of the stabilizer rank method for HOG, sampling $1000000$ amplitudes in the computational basis from the output distribution of Google circuits. We first consider how the runtime and requirements scale as a function of the circuit depth and the precision $\epsilon$, for a $4\times 5$ qubit grid, for $d\in [10, 20]$. We then pick a precision value $\epsilon=0.995$, comparable to that used in~\cite{Villalonga2018}, and explore how the runtime varies with depth for each connectivity pattern, for depth $d\in[15,25]$.\par
Circuits were generated using custom \texttt{C++} code, and interfaced with Python using \texttt{Pybind} in order to run the resulting circuits with \texttt{Qiskit-Aer}. All simulations were run on UCL's Myriad computing cluster, with $2.3\mathrm{GHz}$ processors. The $4\times5$ qubit simulations were run with $36$ parallel workers and $32\mathrm{GB}$ of RAM.\ Due to scheduling restrictions on the cluster, the simulations for differing connectivities were run with $28$ parallel workers, and $28\mathrm{GB}$ of RAM.
\begin{figure}[H]
\centering
\caption{Resource Analysis of Google circuits on a $20$ qubit lattice.}\label{fig:rc_resources}
\begin{subfigure}[t]{0.48\textwidth}
\begin{scaletikzpicturetowidth}{\textwidth}
\input{Figures/RandomCircuits/t_count_with_depth}
\end{scaletikzpicturetowidth}
\caption{Average $T$ count as a function of depth for a $24$ qubit lattice with different connectivity patterns.}\label{fig:t_count_depth}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\begin{scaletikzpicturetowidth}{\textwidth}
\input{Figures/RandomCircuits/t_count_with_lattice}
\end{scaletikzpicturetowidth}
\caption{$T$ count as a function of depth for different sizes of $2$D qubit lattices.}\label{fig:t_count_connectivity}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\begin{scaletikzpicturetowidth}{\textwidth}
\input{Figures/RandomCircuits/max_t_bar}
\end{scaletikzpicturetowidth}
\caption{Plot showing the maximum number of $T$ gates that can be simulated for different system sizes, assuming access to $192\mathrm{GB}$ of RAM.}\label{fig:max_t_bar}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\begin{scaletikzpicturetowidth}{\textwidth}
\input{Figures/RandomCircuits/max_extent_bar}
\end{scaletikzpicturetowidth}
\caption{Plot showing the maximum circuit extent that can be simulated for different system sizes, assuming access to $192\mathrm{GB}$ of RAM.}\label{fig:max_extent_bar}
\end{subfigure}
\end{figure}
\begin{figure}[t]
\centering
\begin{scaletikzpicturetowidth}{0.75\textwidth}
\input{Figures/RandomCircuits/epsilon_runtimes}
\end{scaletikzpicturetowidth}
\caption{Average time required in seconds to take $1000000$ samples from the output distribution of a Google circuit, for different values of the precision $\epsilon$. Also shown is the corresponding $T$ count of the circuit.}\label{fig:precision_runtimes}
\end{figure}
\begin{figure}[t]
\centering
% \begin{scaletikzpicturetowidth}{0.75\textwidth}
% \input{Figures/RandomCircuits/epsilon_runtimes}
% \end{scaletikzpicturetowidth}
\caption{Average time required in seconds to take $1000000$ samples from the output distribution of $1$, $2$ and $3$D versions of a Google circuit. We also consider circuits with All-to-All connectivity, for different fractions of $CZ$ gates $f$.}\label{fig:connectivity_runtimes}
\end{figure}
\section{Discussion}
% Significant Advantages over BG16, and other `general purpise' simulators
We have presented in this chapter a broad range of simulation results, using the techniques introduced in Section~\ref{sec:manipulating_decompositions} and~\ref{sec:implementing_simulator}. These represent the state of the art in simulating quantum circuits using stabilizer state decompositions.\par
As a previously studied benchmark, and as a method with in-built validation, the hidden-shift circuits offer the clearest method to compare our work again the previous results in~\cite{Bravyi2016}. Looking at Figures~\ref{fig:hs_rc_times} it is clear to see the impact of direct decompositions of non-Clifford gates. The small difference in stabilizer fidelity is blown-up by the exponential scaling, resulting in a $7$-fold reduction in the number of terms in the decomposition of $16$ $CCZ$ gates, and a similar reduction in the overall runtime of the simulation.\par
As discussed, decompositions built using the sum-over-Cliffords method require the same number of terms for the $T$ and $CCZ$ gates. However, we might expect it performs better than the gadgetized method when the number of magic states required would be larger than the initial quantum register. In fact, we observe a decrease in runtime using the sum-over-Cliffords method across all values of $\# CCZ$. We also similarly observe smaller gradient in the runtime of the CCZ decompositions\par
The increased performance of the sum-over-Cliffords method is likely due to the greatly simplified preprocessing required; while building the Pauli projector for a PBC is efficient, $O(n^{2})$ run-times can amount to a significant overhead in practice. This improved performance is why the sum-over-Cliffords strategy was also employed for the random circuit simulations, despite these also being based only on $T$ gates.\par
Finally, it is interesting to compare the actually observed error in sampling from the output distributions of circuits, compared to the theoretical bounds. In almost all cases, the decompositions achieved maximum error that was well below the bound of $0.3$ used when fixing the size of the decomposition. The two exceptions were the decompositions built using the random codes method and $T$-gate gadgets, with $52$ and $64$ $T$-gates respectively. This suggests that, despite the probability of picking a valid subspace being large, nonetheless the random subspace method can fail at these problem sizes. In contrast, the smaller decompositions used for the $CCZ$ states, and the sum-over-Clifford methods, all showed relatively consistent error performance across the parameter range.\par
The failures of the random codes method  suggest that for large scale simulations, explicit calculation of the fidelity of the approximation would be necessary. Interestingly however, in the sum-over-Cliffords case, a consequence of the `tail bound' proven in Lemma~7 of~\cite{Bravyi2018} is that, in the case $F\left(\ket{\psi}\right)$ falls off exponentially with the number of copies,
\begin{equation}
\norm{\ket{\psi}-\ket{\tilde{\psi}}}^{2} \leq \braket{\tilde{\psi}} -1 + \epsilon^{2}.
\end{equation}
Thus, using norm estimation to compute $\braket{\tilde{\psi}}$, we can quickly obtain an estimate of the error achieved. Thus, as we move to larger simulations, we can adapt the sum-over-Cliffords simulation method to get better guarantees of the achieved error rate.
\subsection{Simulating NISQ Circuits}
Looking at the plots in Figure~\ref{fig:qaoa_results_comparison}, we can see that the estimates $E_{\gamma}$ obtained using the sum-over-Cliffords method agree closely with the estimates obtained using the methods of~\cite{VandenNest2009}, across the entire parameter range. This serves as a useful validation of the Metropolis method of sampling from the output distribution of the circuit. It is also interesting to note the relative performance of the estimate for both values of the precision $\epsilon$. Importantly, it must be noted that our reference value is itself a classical estimate. Thus, these results cannot be used to ask about the overall error performance of the simulation. However, it is interesting as a means of comparing between the two runs. In general, as expected, we observed the $\epsilon=0.1$ results agree better with the classical estimate, with an average error $\left|E_{sim}-E\right|=0.125$, smaller than the $0.1505$ achieved with $\epsilon=0.15$. However, this comes with an associated $2.25$-fold increase in the number of terms in the decomposition, as shown in Figure~\ref{fig:qaoa_srank}. Due to the aforementioned overhead associated with shared-memory parallelism, this translates to a roughly $2.3$-fold increase in computational runtime.\par
In~\cite{Bravyi2018}, we also presented simulation results for MaxE3Lin2. These simulations were run using only a single thread, and with access to just $8\mathrm{GB}$ of RAM.\ Overall, it took several days to generate the data required. In contrast however, the data shown here was obtained running with $12$ parallel workers and up to $16\mathrm{GB}$ of memory. Taking advantage of this parallelization, the runtime of the simulation is significantly reduced. For example, computing $E(\frac{\pi/8})$, which has the largest stabilizer rank across the parameter range, with $\epsilon=0.15$ required $270$ minutes when running serially. With access to $12$ parallel threads, the same simulation could be completed in just $33$ minutes. In general, we achieved a roughly $8$-fold speedup through parallelization. As previously discussed, computational overhead associated with entering and leaving parallel regions, and portions such as reading and writing files that cannot be parallelised, account for this discrepancy in the number of parallel threads to achieved performance~\cite{Amdahl1967}.\par
These QAOA simulations represent a significant increase in the types of circuits that can be simulated compared to previous sate of the art. As discussed, using $T$ gate synthesis, the number of gates required per rotation $\mathe^{-\mathi\frac{\gamma}{2}Z}$ could vary from $1$ when $\gamma=\pi/4$, to $100$ for $\gamma=1e-8$. Given that a $T$-count of $120$ requires $\sim 370\mathrm{GB}$ of memory, these circuits are only accessible to the sum-over-Cliffords method.\par
In fact, in general the memory requirements of the stabilizer rank method is significantly reduced compared to other methods. For example, a $100$ qubit simulation of the MaxCut problem with qTorch requires $96\mathrm{GB}$ of RAM~\cite{SchuylerFried2017}. Using the sum-over-Cliffords method with $\epsilon=0.15$, we can simulate similar $50$-qubit circuits on a laptop computer with just $8\mathrm{GB}$ of RAM, and would require just $32\mathrm{GB}$ if we extended these simulations to $100$ qubits.\par
In general, circuits targeting NISQ architectures make a good candidate for simulating with the stabilizer rank methods. These circuits are typically limited in depth, and in the number of qubits, meaning the problem sizes stay within ranges accessible to our simulator.\par
Another important aspect of NISQ devices is that noise in the circuit increases with the depth, due to accumulation of individual gate errors. While our simulation method as described does not account for noise directly, we can accordingly relax the target error rate $\epsilon$, which helps to reduce the simulation overhead as the number of qubits or the depth of the circuit grows.\par
Recent work has questioned the computational advantage offered by the QAOA algorithm, even when accounting for extending the algorithm beyond $p=1$ repetitions, by demonstrating classical algorithms which show similar performance~\cite{Hastings2019}. This, coupled with the relatively accessibility of QAOA circuits to our sum-over-Cliffords approach, highlight the importance of considering classical techniques when developing variational quantum algorithms.\par
One example application could be examining the stabilizer extent of different families of ansatz states could potentially be used to rule out classically accessible parameter ranges. If we return to the instance of MaxE3Lin2 considered here, then the parameter value that maximises the expectation, $\frac{\gamma}{\pi}\sim 0.1$, is not the ansatz with maximal extent. This could be taken as an indication of the limitations of QAOA over classical methods, but also shows that large extent does not directly correlate to `computationally interesting'.
\subsection{Simulating Random Circuits}
% Current approach uses naive T expansion
To simulate the Google circuits here, we used the most straightforward approach, decomposing each $T$ gate with the sum-over-Cliffords method using the version of the simulator available in \texttt{Qiskit-Aer}. Unfortunately, as shown in Figure~\ref{fig:rc_resources}, the $T$-count of these circuits grows rapidly in both the depth and the size of the system, this presents a significant limit on the kind of parameter ranges we can explore. While they incorrectly claimed that stabilizer rank necessarily doubles with each non-Clifford gate, the authors of~\cite{Villalonga2019} note that the large non-Clifford gate counts in Google circuits might make them intractable to the stabilizer rank method.\par
Focusing on Figures~\ref{fig:max_t_bar} and~\ref{fig:max_extent_bar}, we can see that the number of qubits in the system only linearly impacts the maximum extent. This is a consequence of the stabilizer state representations developed in Chapter~\ref{chap:stabilizers}. As we pack our representations into $64$-bit integers, up to $n=64$ qubits we see only a linear increase in the spatial complexity, as we require $O(n)$ integers to encode each state. In turn, there is a quadratic dependence on the precision in the decomposition, which we expect from Equation~\ref{eq:extent_srank}.\par
As we are focusing on large NISQ circuits, the fidelity of an experimental realization can be very low as the circuit depth and system size increase~\cite{Villalonga2018,Villalonga2019}, meaning that precision values of $\epsilon=0.995$ are potentially acceptable. These precisions can keep the memory requirements, and correspondingly the runtime, reasonably small. For example, a circuit with $T$-count 100 requires just $9\mathrm{GB}$ of memory to simulate at this precision. They do not, however, prevent the eventual exponential blow up in the circuit extent; with access to the full $192\mathrm{GB}$ of memory available to a compute node on the Myriad cluster, the maximum achievable $T$-count at $\epsilon=9,995$ is still just $\sim. 110$. Using $0.5\mathrm{PB}$ of memory as in~\cite{Villalonga2019} would only add an additional $40$ T-gates to the accessible range.\par
However, the results of Figures~\ref{fig:precision_runtimes} and Fig[insert ref] underline that it is the stabilizer rank, controlled directly by the extent and the desired precision, that is the only significant factor on the runtime of our simulator. This represents a significant potential advantage over \texttt{qFlex} and comparable methods, the runtime of which depends on being able to decompose the circuits into large blocks with as few multi-qubit gates between blocks as possible.\par
In future, it would be interesting to combine these results to examine how the cross-entropy difference behaves as a function of the precision and the connectivity. This would require an ideal realisation of the circuit, but given the circuit sizes considered this could be provided by a vector-based model. While \texttt{Qiskit-Aer} does implement a simulator of this type, access to the underlying state-vector is not yet supported.\par
It would also be interesting to examine if the stabilizer rank method to perform the XEB task. By definition, an approximate stabilizer rank decomposition cannot be used to compute an exact probability $P_{U}\left(\va{x}\right)$; this could be achieved with an exact stabilizer rank expansion, using the results presented in Section~\ref{sec:exact_results}, at the expense of a significant increase in the number of terms required.\par
Alternatively, we could consider computing estimates of $p_{U}\left(\va{x}\right)$. We can compute a computational basis amplitude in time $O\left(\chi_{\epsilon} n^{2}\right)$, but here we also need to reweight the result as
\[\tilde{p}_{U}\left(\va{x}\right)=\frac{1}{\norm{\tilde{\psi}}}\sum_{i}\alpha_{i}\braket{\va{x}}{\phi_{i}},\]
which requires $O\left(\chi_{\epsilon}Ln^{3}\right)$ for L rounds of norm estimation. Recall that to achieve relative error $\delta$, we need $L=4\delta^{-2}$ samples.\par
As Equation~\ref{eq:cross_entropy} depends on terms like $\log\left(p_{U}\left(\va{x}\right)\right)$, a relative error $\delta$ in the norm estimation $\bar{\eta}$ contributes a constant factor $\log\left(1\pm \delta\right)$ as
\[\log{\frac{\left|\braket{\va{x}}{\tilde{\psi}}\right|^{2}}{\bar{\eta}}}=\log\frac{\left|\braket{\va{x}}{\tilde{\psi}}\right|^{2}}{\left(1\pm\delta\right)\norm{\tilde{\psi}}^{2}} = \log\tilde{p}_{U}\left(\va{x}\right) -\log\left(1\pm\delta\right). \]
We also have that our estimates $\tilde{p}\left(\va{x}\right)$ are additive approximations of the true probability with error $\epsilon$. Thus, they contribute to the cross entropy as
\[\log\left(\tilde{p}_{U}\left(\va{x}\right)\right) = \begin{cases}
\log\left(O(\epsilon)\right) + \log\left(\frac{p_{U}}{O(\epsilon)}\pm 1\right) & O(\epsilon) > p_{U} \\
\log\left(p_{U}\right) + \log\left(\frac{O(\epsilon)}{p_{U}}\pm 1\right) & O(\epsilon) < p_{U} \\
\end{cases}\]
Recalling that the Porter-Thomas distribution has significant support on terms with $p_{U}\leq \frac{1}{2^{-n}}$~\cite{Boixo2016}, this would suggest we need to target $\epsilon = O\left(2{-n/2}\right)$ to obtain good estimates of the cross-entropy, and this in turn would imply a stabilizer ranks $O(2{n})$,  suggesting that exact decompositions would likely be better suited to the XEB task.\par
Otherwise, if the stabilizer rank method is to be applied to problems of HOG, how can its performance be improved to access random circuits with greater depth and greater number of qubits? In Section~\ref{sec:general_optimizations}, we will discuss more technical methods that could be used to better scale the simulator to HPC resources, and optimize its resource requirements. Here, we will consider possible methods looking at compiling circuits for the stabilizer rank method.\par
In particular, recent work by Qassim et al.\ introduced a method for recompiling circuits based on sum-over-Clifford expansions of non-Clifford unitaries. As Clifford operators can be written in terms of Pauli rotations as
\[V=\prod_{i} \mathe^{\mathi \theta_{i} P_{i}}\]
for some Pauli operator $P$ and $\theta_{i}$ is a multiple of $\frac{\pi}{4}$. Thus, given a sum-over-Cliffords expansion $U=\sum_{j}\alpha_{j}V_{j}$ of a non-Clifford gate $U$, we can commute a Clifford operator $C$ through it as~\cite{Qassim2019}
\begin{align}
C U = CUC^{\dagger} C &= \left(\sum_{j}\alpha_{j}CVC^{\dagger}\right)C \nonumber \\
&= \left(\sum_{j}\left(\prod_{j} C\mathe^{\mathi\theta_{i,j}P_{i,j}}C^{\dagger} \right)\right)C\nonumber\\
&= \left(\sum_{j}\left(\prod_{j} C\mathe^{\mathi\theta_{i,j}CP_{i,j}C^{\dagger}} \right)\right)C \nonumber \\
&= \left(\sum_{j}\left(\prod_{j} \mathe^{\mathi\theta'_{i,j}P'_{i,j}} \right)\right)C.
\end{align}
Starting with a circuit built up of interleaved Clifford and non-Clifford layers acting on an initial stabilizer state
\[U = C_{m}U_{m}C_{m-1}\cdots C_{1}U_{1}\ket{\phi}\]
Clifford recompilation allows us to commute all Clifford operations through to the beginning of the circuit
\begin{align*}
U &= U_{m}'U_{m-1}'\cdots U_{1}' C_{m}C_{m-1}\cdots C_{1}\ket{\phi} \\
  &= U_{m}'\cdots U_{1}' C'\ket{\phi}\\
  &= U_{m}'\cdots U_{1}'\ket{\phi'},
\end{align*}
where we have used the fact that the input is stabilizer to remove the Clifford terms.
This reduces the runtime of the simulation as we only have to apply the Clifford sequence once, to compute the initial state, rather than applying each operator $\chi$ times for every term in the decomposition.\par
For Google circuits, the recompilation task is made easier as the only non-Clifford gate is already specified as a Pauli rotation, meaning we don't need to first make use of its sum-over-Cliffords expansion. This allows us to rewrite the circuit as a sequence of multi-qubit Pauli rotations acting on an initial stabilizer state.\par
In addition, the authors also show concrete cases where it is possible to build sum-over-Cliffords expansions of products of unitaries $U_{i}U_{k}$ that are `contractive' --- they have smaller extent than the multiplicative expansion~\cite{Qassim2019}. In particular, the authors state that there exists a Clifford circuit $W$ that maps two multi-qubit Pauli operators $P$ and $Q$ to operators $P'$, $Q'$ with support on the same pair of qubits, and that the sum-over-Cliffords expansion is contractive~\cite{Qassim2019}. We provide an explicit description of how to construct such a Clifford circuit $W$ in Algorithm~\ref{alg:clifford_contraction}. As our Clifford-recompiled Google circuit is just a sequence of these rotations, contractive expansions could significantly reduce the stabilizer extent.\par
Thus, applying Clifford recompilation to Google circuits would significantly reduce both the runtime of the simulation, and the value of the extent of the circuit, and thus expanding the parameter space accessible to the stabilizer rank method.
\begin{algorithm}[b]
\begin{algorithmic}
\Require{$n$-qubit Pauli operators $P=\otimes_{i=1}^{n}P_{i}$, $Q=\otimes_{i=1}^{n}Q_{i}$}
\State{Pick qubit $i \; : \; P_{i}=X$ or $Y$.}
\State{$W_{P}\gets I$ \Comment{Initialize empty Clifford circuit}}
\For{$j \neq i\; : \; P_{j}=\{X,Y\}$}
    \State{$W_{P}\gets CNOT_{i,j} W_{P}$ \Comment{$CNOT\left(XX\right)CNOT^{\dagger}=XI$}}
\EndFor
\For{$j \neq i \; P_{j} = \{Z, Y\}$}
    \State{$W_{P}\gets CZ_{i,j}W_{P}$ \Comment{$CZ\left(XZ\right)CZ^{\dagger}=XI$}}
\EndFor
\State{$P'\gets W_{P}PW_{P}^{\dagger}$\Comment{$\left|P'\right| = 1$}}
\State{$Q'\gets W_{P}QW_{P}^{\dagger}$}
\State{Pick qubit $k \neq i \; : \; Q'_{k}=X$ or $Y$.}
\State{$W_{Q}\gets I$ \Comment{Initialize empty Clifford circuit}}
\For{$j\neq  i,k \; : \; Q'_{j}=\{X,Y\}$}
    \State{$W_{Q}\gets CNOT_{i,j} W_{Q}$}
\EndFor
\For{$j \neq i, k \; Q'_{j} = \{Z, Y\}$}
    \State{$W_{Q}\gets CZ_{k,j}W_{Q}$ }
\EndFor
\State{$Q'' = W_{Q}Q'W_{Q}^{\dagger}$\Comment{$\left|Q'\right| \leq 2$}}
\State{\Return{$W= W_{Q}W_{P}$} \Comment{$P'\equiv WPW^{\dagger}\, Q'' = WQW^{\dagger}$ as required.}}
\end{algorithmic}
\caption{Explicit algorithm for constructing a Clifford circuit $W$ that takes two $n$-qubit Pauli operators $P$ and $Q$ and maps them to new operators $P'$, $Q''$ that have support on at most $2$ qubits.}\label{alg:clifford_contraction}
\end{algorithm}
\subsection{Optimizing Decompositions and Sampling}\label{sec:general_optimizations}
% Combining terms to reduce memory footprint
% Precompute samples, store using a tree
% Width of sampled tree => decomposition size?
% Clever distribution to increase shared clifford fraction
Finally, there are several strategies that could be used to reduce the memory requirements and otherwise improve the scalability of the sum-over-Cliffords simulations discussed previously. We focus only on sum-over-Cliffords here as this method is substantially more versatile than the gadgetized methods, and showed better performance overall.\par
Firstly, and recalling the discussion at the end of Section~\ref{sec:srank_discussion}, the sampling method used to build a sum-over-Cliffords decomposition can generate multiple copies of the same state with non-zero probability. In current implementations, samples are taken a gate at a time, independently, and typically across multiple parallel threads, and so there is no deterministic way to check if a given term previously exists in the decomposition without sacrificing parallelization. Additionally, these inclusion checks would incur an additional cost of $O(\chi' n^{2})$, where here we denote $\chi' < \chi$ as the number of terms in the decomposition obtained by grouping states.\par
One possible strategy then would to be precompute the samples for each non-Clifford gate in the circuit, and store this path. For a circuit with $m$ Clifford gates, checking equality of two paths with require time $O(m)$ rather than $O(n^{2})$. Sampling sum-over-Cliffords paths in this way would also enable us to optimize building stabilizer state decompositions. For example, if we have two paths $s, s'$ that are equal for the first $c$ Clifford gates, we can first compute $V_{c}V_{c-1}\dots V_{1}\ket{\phi}$, then copy this state and use it as the input for the remaining fractions of the Clifford circuits.\par
Similar strategies could be employed to produce multiple samples using the Norm Estimation method. For example, say we want to take $m$ samples from the full output distribution of a circuit. We begin by computing $P(x_{0}=0)$, and we sample bits $0$ or $1$ $m$ times using the result. Say now we obtained $a$ strings with $x_{0}=0$. We now compute the next probability, $P\left(x_{0}=0,x_{1}=0\right)$, which together with the previous result allows us to sample from the distribution $P\left(x_{1}=0|x_{0}=0\right)$. We take $a$ samples, and repeat this process for the next bit. This method has the advantage that we do not need to run the full $O\left(\chi n^{6}\right)$ norm estimation step to obtain every sample. Instead, we build up multiple samples one bit at a time.\par
Finally, it is also interesting to note that as our simulation method can also produce estimates of output probabilities $\tilde{p}_{U}\left(\va{x}\right)$, including for subsets of qubits using the norm estimation routine, the rejection sampling method of~\cite{Villalonga2018} should in principle also be implementable with our simulator.