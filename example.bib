@Book{NielsenChuang,
  title     = {Quantum Computation and Quantum Information: 10th Anniversary Edition},
  publisher = {Cambridge University Press},
  year      = {2011},
  author    = {Nielsen, Michael A. and Chuang, Isaac L.},
  isbn      = {1107002176, 9781107002173},
  url       = {https://www.cambridge.org/core/books/quantum-computation-and-quantum-information/01E10196D0A682A6AEFFEA52D53BE9AE},
}

@Book{Watrous2018,
  title     = {The Theory of Quantum Information},
  publisher = {Cambridge University Press},
  year      = {2018},
  author    = {Watrous, John},
  doi       = {10.1017/9781316848142},
  place     = {Cambridge},
  url       = {https://www.cambridge.org/core/books/theory-of-quantum-information/AE4AA5638F808D2CFEB070C55431D897},
}

@Article{Hanneke2016,
  author     = {Hanneke, Steve},
  title      = {The Optimal Sample Complexity of {PAC} Learning},
  journal    = {J. Mach. Learn. Res.},
  year       = {2016},
  volume     = {17},
  number     = {1},
  pages      = {1319--1333},
  month      = {jan},
  issn       = {1532-4435},
  acmid      = {2946683},
  issue_date = {January 2016},
  keywords   = {PAC learning, learning algorithm, minimax analysis, sample complexity, statistical learning theory},
  numpages   = {15},
  publisher  = {JMLR.org},
  url        = {http://dl.acm.org/citation.cfm?id=2946645.2946683},
}
@Article{Valiant:1984,
  author     = {Valiant, L. G.},
  title      = {A Theory of the Learnable},
  journal    = {Commun. ACM},
  year       = {1984},
  volume     = {27},
  number     = {11},
  pages      = {1134--1142},
  month      = nov,
  issn       = {0001-0782},
  acmid      = {1972},
  address    = {New York, NY, USA},
  doi        = {10.1145/1968.1972},
  issue_date = {Nov. 1984},
  keywords   = {inductive inference, probabilistic models of learning, propositional expressions},
  numpages   = {9},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1968.1972},
}

@Misc{deWolfQIP2018,
  author = {Ronald de Wolf},
  title  = {Quantum Learning Theory},
  year   = {2018},
  note   = {\emph{Quantum Information Processing} 2018},
  url    = {https://qutech.nl/wp-content/uploads/2018/01/QIP18MLtutorial_Ronald-de-Wolf.pdf},
}

@Article{Farhi2018,
  author      = {Edward Farhi and Hartmut Neven},
  title       = {Classification with Quantum Neural Networks on Near Term Processors},
  year        = {2018},
  abstract    = {We introduce a quantum neural network, QNN, that can represent labeled data, classical or quantum, and be trained by supervised learning. The quantum circuit consists of a sequence of parameter dependent unitary transformations which acts on an input quantum state. For binary classification a single Pauli operator is measured on a designated readout qubit. The measured output is the quantum neural network's predictor of the binary label of the input state. First we look at classifying classical data sets which consist of n-bit strings with binary labels. The input quantum state is an n-bit computational basis state corresponding to a sample string. We show how to design a circuit made from two qubit unitaries that can correctly represent the label of any Boolean function of n bits. For certain label functions the circuit is exponentially long. We introduce parameter dependent unitaries that can be adapted by supervised learning of labeled data. We study an example of real world data consisting of downsampled images of handwritten digits each of which has been labeled as one of two distinct digits. We show through classical simulation that parameters can be found that allow the QNN to learn to correctly distinguish the two data sets. We then discuss presenting the data as quantum superpositions of computational basis states corresponding to different label values. Here we show through simulation that learning is possible. We consider using our QNN to learn the label of a general quantum state. By example we show that this can be done. Our work is exploratory and relies on the classical simulation of small quantum systems. The QNN proposed here was designed with near-term quantum processors in mind. Therefore it will be possible to run this QNN on a near term gate model quantum computer where its power can be explored beyond what can be explored with simulation.},
  arxivid     = {1802.06002},
  date        = {2018-02-16},
  eprintclass = {quant-ph},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1802.06002v2:PDF},
  keywords    = {quant-ph},
}

@article{Nayak2000,
abstract = {Motivated by the immense success of random walk and Markov chain methods in the design of classical algorithms, we consider_quantum_ walks on graphs. We analyse in detail the behaviour of unbiased quantum walk on the line, with the example of a typical walk, the ``Hadamard walk''. We show that after t time steps, the probability distribution on the line induced by the Hadamard walk is almost uniformly distributed over the interval [-t/sqrt(2),t/sqrt(2)]. This implies that the same walk defined on the circle mixes in_linear_ time. This is in direct contrast with the quadratic mixing time for the corresponding classical walk. We conclude by indicating how our techniques may be applied to more general graphs.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0010117},
author = {Nayak, Ashwin and Vishwanath, Ashvin},
eprint = {0010117},
file = {:Users/danial/Library/Application Support/Mendeley Desktop/Downloaded/Nayak, Vishwanath - 2000 - Quantum Walk on the Line.pdf:pdf},
month = {oct},
primaryClass = {arXiv:quant-ph},
title = {{Quantum Walk on the Line}},
url = {http://arxiv.org/abs/quant-ph/0010117},
year = {2000}
}